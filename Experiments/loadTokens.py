from BPETokenizer import BPETokenizer as Tokenizer
from BPEencoding import prepare_documents_from_dataset
from config import *
from tqdm import tqdm
import pickle
import torch
from datasets import load_dataset


def encodeAndSaveTokens(config, mode="train"):

    tokenizer = Tokenizer(config)

    # åŠ è½½ OpenWebText æ•°æ®é›†ï¼ˆè‹¥å·²æœ¬åœ°ç¼“å­˜ï¼Œload_dataset ä¼šæ›´å¿«ï¼‰
    print("ğŸš€ åŠ è½½ OpenWebText æ•°æ®é›†...")
    dataset = load_dataset("sytelus/openwebtext")

    # å‡†å¤‡è®­ç»ƒæ–‡æ¡£
    original_data_list = prepare_documents_from_dataset(
        dataset,
        split=mode,
       sample_size= None, # Noneè¡¨ç¤ºå…¨éƒ¨
        text_column="text",
    )

    encode_ids_list = []
    eos_token_id = tokenizer.config["special_tokens"][0]  # å‡è®¾tokenizeræœ‰è¯¥å±æ€§
    if eos_token_id==None:
        eos_token_id="<|endoftext|>"

    # å¯¹æ¯ä¸ªæ–‡æ¡£ç¼–ç ï¼Œå¹¶æ·»åŠ eosæ ‡è®°
    for doc in tqdm(original_data_list, desc=f"Encoding {mode} data"):
        if not doc.strip():  # è·³è¿‡ç©ºæ–‡æ¡£
            continue
        ids = tokenizer.encode(doc)
        encode_ids_list.extend(ids)
        encode_ids_list.append(eos_token_id)  # æ–‡æ¡£ç»“æŸæ ‡è®°

    encode_ids = torch.tensor(encode_ids_list, dtype=torch.long)

    return encode_ids


# ç¼–ç éªŒè¯æ•°æ®

print("å¼€å§‹ç¼–ç è®­ç»ƒæ•°æ®...")
train_encode_ids = encodeAndSaveTokens(config,mode="train")
print(f"âœ…è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆï¼Œtokens={len(train_encode_ids)}")

train_encode_ids_path = config["train_encode_ids_path"]
with open(train_encode_ids_path, "wb") as f:
    pickle.dump(train_encode_ids, f)
print(f"ğŸ’¾è®­ç»ƒæ•°æ®ç¼–ç å®Œæˆå¹¶å·²ä¿å­˜åˆ°{train_encode_ids_path}\n")


print("å¼€å§‹ç¼–ç éªŒè¯æ•°æ®...")
valid_encode_ids = encodeAndSaveTokens(config,mode="valid")
print(f"âœ…éªŒè¯æ•°æ®åŠ è½½å®Œæˆï¼Œtokens={len(valid_encode_ids)}")

valid_encode_ids_path = config["valid_encode_ids_path"]
with open(valid_encode_ids_path, "wb") as f:
    pickle.dump(valid_encode_ids, f)
print(f"ğŸ’¾éªŒè¯æ•°æ®ç¼–ç å®Œæˆå¹¶å·²ä¿å­˜åˆ°{valid_encode_ids_path}\n")
