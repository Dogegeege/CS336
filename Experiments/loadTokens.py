from BPETokenizer import BPETokenizer as Tokenizer
from BPEencoding import prepare_documents_from_dataset
from config import *
from tqdm import tqdm
import pickle
import torch
from datasets import load_dataset


def encodeAndSaveTokens(config, mode="train"):
    tokenizer = Tokenizer(config)

    # ä»é…ç½®ä¸­è·å–æ ·æœ¬å¤§å°ï¼Œè‹¥æ— åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼ˆä¾‹å¦‚ 10000ï¼‰
    sample_size = config.get(f"{mode}_sample_size", 10000)

    print(f"ğŸš€ åŠ è½½ OpenWebText æ•°æ®é›†ä½œä¸º {mode} æ•°æ®ï¼ˆå‰ {sample_size} ç¯‡ï¼‰...")

    # åŠ è½½æ•°æ®é›†æ—¶ç›´æ¥åˆ‡ç‰‡ï¼Œé¿å…ä¸‹è½½å…¨éƒ¨
    if mode == "train":
        # è®­ç»ƒé›†ï¼šä»å¤´å¼€å§‹å– sample_size ç¯‡
        dataset = load_dataset(
            "sytelus/openwebtext",
            split=f"train[:{sample_size}]"
        )
        print(f"âœ… è®­ç»ƒé›†åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} ç¯‡æ–‡æ¡£")
    else:
        # éªŒè¯é›†ï¼šä»è®­ç»ƒé›†æœ«å°¾ä¹‹åå¼€å§‹å– sample_size ç¯‡
        # éœ€è¦çŸ¥é“è®­ç»ƒé›†çš„å¤§å°ï¼Œä»¥ä¾¿åç§»èµ·å§‹ç´¢å¼•
        train_size = config.get("valid_sample_size", 1000)
        start = train_size
        end = start + sample_size
        dataset = load_dataset(
            "sytelus/openwebtext",
            split=f"train[{start}:{end}]"
        )
        print(f"âœ… éªŒè¯é›†åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} ç¯‡æ–‡æ¡£")

    # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨ï¼ˆå‡è®¾ prepare_documents_from_dataset èƒ½ç›´æ¥å¤„ç† dataset å¹¶æå– text åˆ—ï¼‰
    original_data_list = prepare_documents_from_dataset(
        dataset,
        split=mode,
        sample_size=None,  # å› ä¸º dataset å·²ç»åˆ‡ç‰‡ï¼Œæ­¤å¤„æ— éœ€å†é™åˆ¶
        text_column="text",
    )

    # è·å– eos_token_idï¼ˆæ¨èç›´æ¥ä» tokenizer å¯¹è±¡è·å–ï¼‰
    eos_token_id = tokenizer.special_to_id["<|endoftext|>"]  # æ ¹æ®æ‚¨çš„ Tokenizer å®ç°è°ƒæ•´
    # å¦‚æœä¸Šè¿°æ–¹æ³•ä¸å¯ç”¨ï¼Œå¯ç¡¬ç¼–ç ï¼šeos_token_id = 50256ï¼ˆGPT-2 çš„ eos å€¼ï¼‰

    encode_ids_list = []

    # é€ä¸ªæ–‡æ¡£ç¼–ç ï¼Œæ·»åŠ  eos æ ‡è®°
    for doc in tqdm(original_data_list, desc=f"Encoding {mode} data"):
        if not doc.strip():
            continue
        ids = tokenizer.encode(doc)
        encode_ids_list.extend(ids)
        encode_ids_list.append(eos_token_id)

    encode_ids = torch.tensor(encode_ids_list, dtype=torch.long)

    # å¯é€‰ï¼šä¿å­˜åˆ°é…ç½®æŒ‡å®šçš„è·¯å¾„
    save_path = config.get(f"{mode}_encode_ids_path")
    if save_path:
        with open(save_path, "wb") as f:
            pickle.dump(encode_ids, f)
        print(f"âœ… å·²ä¿å­˜ç¼–ç åçš„ token ids åˆ° {save_path}")

    return encode_ids


# ç¼–ç éªŒè¯æ•°æ®

print("å¼€å§‹ç¼–ç è®­ç»ƒæ•°æ®...")
train_encode_ids = encodeAndSaveTokens(config,mode="train")
print(f"âœ…è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆï¼Œtokens={len(train_encode_ids)}")

train_encode_ids_path = config["train_encode_ids_path"]
with open(train_encode_ids_path, "wb") as f:
    pickle.dump(train_encode_ids, f)
print(f"ğŸ’¾è®­ç»ƒæ•°æ®ç¼–ç å®Œæˆå¹¶å·²ä¿å­˜åˆ°{train_encode_ids_path}\n")


print("å¼€å§‹ç¼–ç éªŒè¯æ•°æ®...")
valid_encode_ids = encodeAndSaveTokens(config,mode="valid")
print(f"âœ…éªŒè¯æ•°æ®åŠ è½½å®Œæˆï¼Œtokens={len(valid_encode_ids)}")

valid_encode_ids_path = config["valid_encode_ids_path"]
with open(valid_encode_ids_path, "wb") as f:
    pickle.dump(valid_encode_ids, f)
print(f"ğŸ’¾éªŒè¯æ•°æ®ç¼–ç å®Œæˆå¹¶å·²ä¿å­˜åˆ°{valid_encode_ids_path}\n")
