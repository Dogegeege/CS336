from BPEencoding import BPETokenizer as Tokenizer
from config import *
from tqdm import tqdm


def encodeAndSaveTokens(config, data_path):

    tokenizer = Tokenizer(config)

    # åŠ è½½è®­ç»ƒæ•°æ®
    with open(data_path, "r", encoding="utf-8") as f:
        original_data = f.read()

    chunk_size = 10000  # æ¯å—å­—ç¬¦æ•°ï¼Œå¯æ ¹æ®å†…å­˜/é€Ÿåº¦è°ƒæ•´
    encode_ids_list = []

    # åˆ†å—ç¼–ç å¹¶æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆé¿å…ä¸€æ¬¡æ€§å¯¹è¶…é•¿æ–‡æœ¬ç¼–ç ï¼‰
    if len(original_data) <= chunk_size:
        # å¦‚æœæ–‡æœ¬è¾ƒçŸ­ï¼Œç›´æ¥ä¸€æ¬¡æ€§ç¼–ç 
        encode_ids_list = tokenizer.encode(original_data)
    else:
        for i in tqdm(
            range(0, len(original_data), chunk_size), desc="ğŸ”„Encoding", unit="chunk"
        ):
            chunk = original_data[i : i + chunk_size]
            encode_ids_list.extend(tokenizer.encode(chunk))

    encode_ids = torch.tensor(encode_ids_list, dtype=torch.long)

    return encode_ids


# ç¼–ç éªŒè¯æ•°æ®

print("å¼€å§‹ç¼–ç è®­ç»ƒæ•°æ®...")
data_path = config["train_data_path"]
train_encode_ids = encodeAndSaveTokens(config, data_path)
print(f"âœ…è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆï¼Œtokens={len(train_encode_ids)}")

train_encode_ids_path = config["train_encode_ids_path"]
with open(train_encode_ids_path, "wb") as f:
    pickle.dump(train_encode_ids, f)
print(f"ğŸ’¾è®­ç»ƒæ•°æ®ç¼–ç å®Œæˆå¹¶å·²ä¿å­˜åˆ°{train_encode_ids_path}\n")


print("å¼€å§‹ç¼–ç éªŒè¯æ•°æ®...")
data_path = config["valid_data_path"]
valid_encode_ids = encodeAndSaveTokens(config, data_path)
print(f"âœ…éªŒè¯æ•°æ®åŠ è½½å®Œæˆï¼Œtokens={len(valid_encode_ids)}")

valid_encode_ids_path = config["valid_encode_ids_path"]
with open(valid_encode_ids_path, "wb") as f:
    pickle.dump(valid_encode_ids, f)
print(f"ğŸ’¾éªŒè¯æ•°æ®ç¼–ç å®Œæˆå¹¶å·²ä¿å­˜åˆ°{valid_encode_ids_path}\n")
