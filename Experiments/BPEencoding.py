import os
import heapq
import regex
import time
import random
import multiprocessing
from functools import partial
from tqdm import tqdm
from pathlib import Path
from typing import List, Tuple, Dict, DefaultDict, Any, Union
import mmap
import re
from collections import defaultdict
import base64
import config as global_config
from datasets import load_dataset
from multiprocessing.pool import ThreadPool


# GPT-2é¢„åˆ†è¯æ¨¡å¼
GPT2_SPLIT_PATTERN = (
    r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
)
MAX_PROCESSES = multiprocessing.cpu_count()


def load_and_sample_data(
    file_path: str, sample_size: int = 22000, special_token: str = "<|endoftext|>"
) -> str:
    """å†…å­˜æ˜ å°„æ–¹å¼åŠ è½½å¹¶é‡‡æ ·æ–‡æ¡£"""
    try:
        with open(file_path, "r+", encoding="utf-8", errors="ignore") as f:
            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                documents = []
                start = 0
                while start < len(mm):
                    end = mm.find(special_token.encode("utf-8"), start)
                    if end == -1:
                        doc = mm[start:].decode("utf-8", errors="replace").strip()
                        if doc:
                            documents.append(doc)
                        break
                    else:
                        doc = mm[start:end].decode("utf-8", errors="replace").strip()
                        if doc:
                            documents.append(doc)
                    start = end + len(special_token)

                # å¦‚æœæ–‡æ¡£é•¿åº¦è¶…è¿‡é‡‡æ ·å¤§å°ï¼Œåˆ™éšæœºé‡‡æ ·
                if len(documents) > sample_size:
                    documents = random.sample(documents, sample_size)

                return special_token.join(documents)
    except Exception as e:
        raise IOError(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")


def gpt2_bytes_to_unicode_local() -> Dict[int, str]:
    """å­—èŠ‚åˆ°Unicodeæ˜ å°„"""
    # åˆ—è¡¨åŒ…å«ASCIIå¯æ‰“å°å­—ç¬¦ï¼ˆ33-126ï¼‰å’Œæ‰©å±•å­—ç¬¦é›†ï¼ˆ161-172, 174-255ï¼‰ï¼Œè¦†ç›–å¸¸è§å­—ç¬¦å’Œç‰¹æ®Šç¬¦å·ã€‚
    # å…¶ä»–å­—èŠ‚ï¼ˆæ¯”å¦‚ä¸­æ–‡ï¼‰æ˜ å°„åˆ°256åŠä»¥ä¸Šçš„Unicodeç ä½ï¼Œç¡®ä¿æ¯ä¸ªå­—èŠ‚éƒ½æœ‰å”¯ä¸€å¯¹åº”ã€‚

    # TODO: æ‰©å±•ä¸­æ–‡å­—ç¬¦é›†
    bs = list(range(33, 127)) + list(range(161, 173)) + list(range(174, 256))
    cs = bs[:]
    n = 0
    for b in range(256):
        if b not in bs:
            bs.append(b)
            cs.append(256 + n)
            n += 1
    return {b: chr(c) for b, c in zip(bs, cs)}


def pre_tokenize_document(
    doc: str, bytes_to_unicode_map: Dict[int, str]
) -> List[List[str]]:
    """é¢„åˆ†è¯å¤„ç†å•ä¸ªæ–‡æ¡£"""
    # åˆ†å‰²åçš„åŸå§‹å­—ç¬¦ä¸²ç‰‡æ®µï¼Œä¾‹å¦‚ï¼š"hello"ã€" world"ã€"!"ã€"ca n't" ç­‰
    tokens = regex.findall(GPT2_SPLIT_PATTERN, doc, flags=regex.UNICODE)
    sequences = []
    for token in tokens:
        # é€å­—èŠ‚æ˜ å°„ä¸ºUnicodeå­—ç¬¦ï¼Œå°†Unicodeå­—ç¬¦ä¸²ç¼–ç ä¸ºUTF-8å­—èŠ‚åºåˆ—ã€‚

        # å°†å˜é•¿å­—ç¬¦è½¬æ¢ä¸ºå›ºå®š1å­—èŠ‚çš„å•ä½ï¼ˆ0-255èŒƒå›´ï¼‰ã€‚ä¾‹å¦‚ï¼š
        # å­—ç¬¦ä¸² "hello" â†’ å­—èŠ‚åºåˆ— [104, 101, 108, 108, 111]
        # ä¸­æ–‡å­—ç¬¦ä¸² "ä½ å¥½" â†’ å­—èŠ‚åºåˆ— [228, 189, 160, 229, 165, 189]
        # è¿™æ ·ï¼ŒBPEå¯ä»¥ç›´æ¥åœ¨å­—èŠ‚çº§åˆ«è¿›è¡Œåˆå¹¶ï¼Œè€Œä¸éœ€è¦å¤„ç†å˜é•¿ç¼–ç çš„å¤æ‚æ€§ã€‚
        # å­—èŠ‚æ˜¯åŸå­å•ä½ï¼Œæ¯ä¸ªå­—èŠ‚ä»£è¡¨ä¸€ä¸ªå›ºå®šå¤§å°çš„â€œåŸºæœ¬å•å…ƒâ€ã€‚

        # ? ä¸ºä»€ä¹ˆå†æ˜ å°„å›Unicodeï¼Ÿ
        # ç›´æ¥åœ¨å­—èŠ‚ä¸Šæ“ä½œæ—¶ï¼Œå­—èŠ‚å€¼æ˜¯0-255çš„æ•´æ•°ï¼Œä¸ä¾¿äºè§‚å¯Ÿå’Œå¤„ç†ï¼ˆå› ä¸ºè®¸å¤šå­—èŠ‚å¯¹åº”ä¸å¯è§å­—ç¬¦æˆ–æ§åˆ¶å­—ç¬¦ï¼‰ã€‚
        # åœ¨ gpt2_bytes_to_unicode_local å‡½æ•°ä¸­ï¼Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªæ˜ å°„è¡¨ï¼Œå°†æ¯ä¸ªå­—èŠ‚ï¼ˆ0-255ï¼‰æ˜ å°„åˆ°ä¸€ä¸ªå”¯ä¸€çš„Unicodeå­—ç¬¦ï¼š
        # ASCIIå¯æ‰“å°å­—ç¬¦ï¼ˆ33-126ï¼Œå¦‚å­—æ¯ã€æ•°å­—ã€ç¬¦å·ï¼‰ä¿æŒä¸å˜ã€‚
        # å…¶ä»–å­—èŠ‚ï¼ˆå¦‚ä¸å¯è§å­—ç¬¦æˆ–æ‰©å±•å­—ç¬¦ï¼‰æ˜ å°„åˆ°256åŠä»¥ä¸Šçš„Unicodeç ä½ï¼ˆä½¿ç”¨ chr(256 + n)ï¼‰ï¼Œç¡®ä¿æ¯ä¸ªå­—èŠ‚éƒ½æœ‰ä¸€ä¸ªå¯è§çš„ã€å¯æ‰“å°çš„Unicodeä»£ç†å­—ç¬¦ã€‚
        # ä¾‹å¦‚ï¼Œå­—èŠ‚ 0ï¼ˆç©ºå­—ç¬¦ï¼‰å¯èƒ½æ˜ å°„åˆ°æŸä¸ªç‰¹æ®ŠUnicodeå­—ç¬¦ï¼Œå¦‚ 'Ä€' æˆ–ç±»ä¼¼ã€‚
        # åœ¨ pre_tokenize_document ä¸­ï¼Œæ¯ä¸ªå­—èŠ‚è¢«è½¬æ¢ä¸ºå¯¹åº”çš„Unicodeå­—ç¬¦ï¼štoken_unicode = "".join(bytes_to_unicode_map[b] for b in token.encode("utf-8"))
        # è¿™å°†å­—èŠ‚åºåˆ—è½¬æ¢ä¸ºUnicodeå­—ç¬¦ä¸²ï¼Œä½†æ¯ä¸ªâ€œå­—ç¬¦â€å®é™…ä¸Šä»£è¡¨ä¸€ä¸ªå­—èŠ‚ã€‚
        # ä¾‹å¦‚ï¼Œ"hello" çš„å­—èŠ‚ [104, 101, 108, 108, 111] æ˜ å°„ä¸º ['h', 'e', 'l', 'l', 'o']ï¼ˆå¦‚æœ104å¯¹åº”'h'ï¼‰ã€‚
        # *å¯¹äºä¸­æ–‡ï¼Œå­—èŠ‚åºåˆ—ä¼šè¢«æ˜ å°„ä¸ºä¸€ç³»åˆ—å¯è§çš„ä»£ç†å­—ç¬¦ï¼Œä¾¿äºBPEåœ¨è¿™äº›â€œå­—ç¬¦â€ä¸Šè¿›è¡Œåˆå¹¶
        token_unicode = "".join(bytes_to_unicode_map[b] for b in token.encode("utf-8"))
        sequences.append(list(token_unicode))
    # è¿”å›å½¢å¼å½¢å¦‚: [['H', 'e', 'l', 'l', 'o'],[','],[' ', 'w', 'o', 'r', 'l', 'd'],['!']]
    return sequences


def pre_tokenize_worker(
    doc: str, bytes_to_unicode_map: Dict[int, str]
) -> List[List[str]]:
    """ä¾› multiprocessing è°ƒç”¨çš„é¡¶å±‚å‡½æ•°"""
    return pre_tokenize_document(doc, bytes_to_unicode_map)


def parallel_pre_tokenize(
    documents: List[str], num_processes: int, bytes_to_unicode_map: Dict[int, str]
) -> List[List[str]]:
    """å¹¶è¡Œé¢„åˆ†è¯ä¼˜åŒ–"""
    if num_processes <= 1:
        return [
            seq
            for doc in documents
            for seq in pre_tokenize_document(doc, bytes_to_unicode_map)
        ]

    from functools import partial

    # ç»‘å®šå­—èŠ‚æ˜ å°„è¡¨ï¼Œç”Ÿæˆä¸€ä¸ªä»…æ¥æ”¶ doc çš„å¯è°ƒç”¨å¯¹è±¡
    worker = partial(pre_tokenize_worker, bytes_to_unicode_map=bytes_to_unicode_map)

    with multiprocessing.Pool(num_processes) as pool:
        results = list(
            tqdm(
                pool.imap(worker, documents, chunksize=50),
                total=len(documents),
                desc="é¢„åˆ†è¯",
                mininterval=1,
            )
        )
    return [seq for doc_sequences in results for seq in doc_sequences]


class BPEIndex:
    """é«˜æ•ˆç´¢å¼•ç»“æ„ç”¨äºBPEåˆå¹¶"""

    def __init__(self, sequences: List[List[str]]):
        self.sequences = sequences  # å­˜å‚¨æ‰€æœ‰æ–‡æœ¬åºåˆ—
        self.pair_counts: DefaultDict[Tuple[str, str], int] = defaultdict(
            int
        )  # ç»Ÿè®¡å­—èŠ‚å¯¹é¢‘ç‡
        self.pair_positions: DefaultDict[Tuple[str, str], List[Tuple[int, int]]] = (
            defaultdict(list)
        )  # è®°å½•å­—èŠ‚å¯¹ä½ç½®
        self.heap = []  # æœ€å¤§å †ï¼ˆå­˜æœ€é«˜é¢‘å­—èŠ‚å¯¹ï¼‰
        self.heap_entries: Dict[Tuple[str, str], Any] = {}  # å †æ¡ç›®å¿«é€Ÿè®¿é—®

        # åˆå§‹åŒ–ç´¢å¼• ä¸€æ¬¡æ€§ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„å‡ºç°ä½ç½®å’Œé¢‘ç‡â€”â€”å°†ä¸å¯è¡Œçš„O(NÂ²)é—®é¢˜è½¬åŒ–ä¸ºå¯å¤„ç†çš„O(N log N)
        for seq_idx, seq in enumerate(sequences):
            for pos in range(len(seq) - 1):
                pair = (seq[pos], seq[pos + 1])
                self.pair_counts[pair] += 1
                self.pair_positions[pair].append((seq_idx, pos))

        # æ„å»ºå † å°†é«˜é¢‘å­—èŠ‚å¯¹ï¼ˆ>1æ¬¡ï¼‰åŠ å…¥æœ€å¤§å †ï¼Œè®© get_most_frequent() èƒ½ O(1) è·å–æœ€é«˜é¢‘å¯¹ã€‚
        for pair, count in self.pair_counts.items():
            if count > 1:  # åªæ·»åŠ è®¡æ•°å¤§äº1çš„pair
                entry = [-count, pair]
                heapq.heappush(self.heap, entry)  # å †é‡æ„æ•°ç»„ï¼ˆå°æ ¹å †ï¼‰
                self.heap_entries[pair] = entry

    def get_most_frequent(self) -> Tuple[str, str]:
        """å¿«é€Ÿè¿”å›å½“å‰æœ€é«˜é¢‘å­—èŠ‚å¯¹ï¼ˆè·³è¿‡å·²è¢«åˆå¹¶çš„æ— æ•ˆæ¡ç›®ï¼‰"""
        while self.heap:
            neg_count, pair = self.heap[0]
            # æ£€æŸ¥pairæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            if pair not in self.heap_entries:
                heapq.heappop(self.heap)
                continue

            current_count = self.pair_counts.get(pair, 0)

            # æ£€æŸ¥è®¡æ•°æ˜¯å¦åŒ¹é…ä¸”å¤§äº1
            if -neg_count == current_count and current_count > 1:
                return pair
            # å¦åˆ™ç§»é™¤æ— æ•ˆæ¡ç›®
            heapq.heappop(self.heap)
            if pair in self.heap_entries:  # ç¡®ä¿æ¡ç›®å­˜åœ¨
                del self.heap_entries[pair]
        return None

    def merge_pair(self, pair: Tuple[str, str], new_token: str) -> int:
        """åˆå¹¶å­—ç¬¦å¯¹å¹¶æ›´æ–°ç´¢å¼•"""
        if pair not in self.pair_positions or not self.pair_positions[pair]:
            return 0

        # å°†å­—èŠ‚å¯¹æŒ‰åºåˆ—åˆ†ç»„
        positions_by_seq = defaultdict(list)
        for seq_idx, pos in self.pair_positions[pair]:
            positions_by_seq[seq_idx].append(pos)

        merge_count = 0
        # éå†åˆ†ç»„
        for seq_idx, positions in positions_by_seq.items():
            seq = self.sequences[seq_idx]  # æµ…æ‹·è´
            # æŒ‰ä½ç½®å€’åºæ’åº
            positions.sort(reverse=True)
            last_merged_pos = -2

            for pos in positions:
                # æ£€æŸ¥æ˜¯å¦å·²è¢«å‰é¢çš„åˆå¹¶å½±å“
                if pos >= len(seq) - 1 or pos <= last_merged_pos:
                    continue  # è·³è¿‡å·²åˆå¹¶ä½ç½®
                if seq[pos] != pair[0] or seq[pos + 1] != pair[1]:
                    continue  # åªåˆå¹¶å®Œå…¨åŒ¹é…çš„pair

                # æ‰§è¡Œåˆå¹¶
                seq[pos] = new_token
                del seq[pos + 1]
                merge_count += 1
                last_merged_pos = pos

                # æ›´æ–°å·¦ä¾§pair, (A, B) -> (A, new_token)
                if pos > 0:
                    left_pair = (seq[pos - 1], pair[0])
                    self._update_pair_count(left_pair, -1)

                    new_left_pair = (seq[pos - 1], new_token)
                    self._update_pair_count(new_left_pair, 1)
                    self._add_position(new_left_pair, seq_idx, pos - 1)

                # æ›´æ–°å³ä¾§pair , (B, C) -> (new_token, C)
                if pos < len(seq) - 1:
                    right_pair = (pair[1], seq[pos + 1])
                    self._update_pair_count(right_pair, -1)

                    new_right_pair = (new_token, seq[pos + 1])
                    self._update_pair_count(new_right_pair, 1)
                    self._add_position(new_right_pair, seq_idx, pos)

        # æ¸…ç†å·²åˆå¹¶çš„pair
        if pair in self.pair_counts:
            del self.pair_counts[pair]
        if pair in self.pair_positions:
            del self.pair_positions[pair]
        if pair in self.heap_entries:
            # æ ‡è®°ä¸ºæ— æ•ˆï¼Œç¨åæ¸…ç†
            self.heap_entries[pair] = None

        return merge_count

    def _update_pair_count(self, pair: Tuple[str, str], delta: int):
        """æ›´æ–°å­—ç¬¦å¯¹è®¡æ•°
        æ›´æ–°`pair_counts`çš„è®¡æ•°ï¼Œå¹¶ç»´æŠ¤å †ç»“æ„\n

        pair: éœ€è¦æ›´æ–°çš„å­—ç¬¦å¯¹\n
        delta: è®¡æ•°å¢é‡ï¼ˆæ­£æ•°å¢åŠ ï¼Œè´Ÿæ•°å‡å°‘ï¼‰
        """
        if delta == 0:
            return

        # ç¡®ä¿pairå­˜åœ¨äºå­—å…¸ä¸­
        if pair not in self.pair_counts:
            self.pair_counts[pair] = 0

        new_count = self.pair_counts[pair] + delta
        self.pair_counts[pair] = new_count

        # ç¡®ä¿è®¡æ•°ä¸ä¸ºè´Ÿ
        if new_count < 0:
            new_count = 0
            self.pair_counts[pair] = 0

        if pair in self.heap_entries and self.heap_entries[pair] is not None:
            # æ›´æ–°å †æ¡ç›®
            self.heap_entries[pair][0] = -new_count
            heapq.heapify(self.heap)  # è°ƒæ•´å †
        elif new_count > 1:  # åªæ·»åŠ è®¡æ•°å¤§äº1çš„pair
            # æ–°å»ºå †æ¡ç›®
            entry = [-new_count, pair]
            heapq.heappush(self.heap, entry)
            self.heap_entries[pair] = entry

    def _add_position(self, pair: Tuple[str, str], seq_idx: int, pos: int):
        """æ·»åŠ æ–°ä½ç½®åˆ°ç´¢å¼•"""
        self.pair_positions[pair].append((seq_idx, pos))


def prepare_documents_from_dataset(
    dataset, split="train", sample_size=None, text_column="text"
) -> List[str]:
    """ä» Hugging Face Dataset ä¸­æå–æ–‡æ¡£åˆ—è¡¨ï¼ˆæé€Ÿé‡‡æ ·ç‰ˆï¼‰"""
    # å¤„ç† DatasetDict
    if isinstance(dataset, dict) and split in dataset:
        data = dataset[split]
    else:
        data = dataset

    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    if text_column not in data.column_names:
        raise KeyError(f"æ•°æ®é›†æ²¡æœ‰ '{text_column}' åˆ—ï¼Œå¯ç”¨åˆ—: {data.column_names}")

    # âœ… å…³é”®ä¿®å¤ï¼šç›´æ¥ç”¨ len(data) åˆ¤æ–­ï¼Œä¸éœ€è¦é¢„å…ˆåŠ è½½ texts
    if sample_size is not None and sample_size < len(data):
        indices = random.sample(range(len(data)), sample_size)
        # âš¡ æé€Ÿé‡‡æ ·ï¼šselect åº•å±‚æ˜¯ Arrow åˆ‡ç‰‡ï¼Œæ¯«ç§’çº§
        sampled = data.select(indices)
        texts = sampled[text_column]  # æ­¤æ—¶å·²ç»æ˜¯ Python list
        print(f"ğŸ“š ä»æ•°æ®é›† '{split}' é‡‡æ · {len(texts):,} ç¯‡æ–‡æ¡£")
    else:
        texts = data[text_column]
        if not isinstance(texts, list):
            texts = list(texts)
        print(f"ğŸ“š ä»æ•°æ®é›† '{split}' åŠ è½½å…¨éƒ¨ {len(texts):,} ç¯‡æ–‡æ¡£")

    return texts


def run_train_bpe(
    documents: List[str],  # ç›´æ¥æ¥æ”¶æ–‡æ¡£åˆ—è¡¨
    vocab_size: int,
    special_tokens: List[str] = ["<|endoftext|>"],
    num_processes: int = 8,
    **kwargs,
) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:
    """è¿è¡ŒBPEè®­ç»ƒæµç¨‹
    Args:
        documents: æ–‡æ¡£å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå·²é¢„å…ˆåˆ†å¥½ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ç¯‡å®Œæ•´æ–‡æ¡£ï¼‰
        vocab_size: ç›®æ ‡è¯æ±‡è¡¨å¤§å°
        special_tokens: ç‰¹æ®Štokenåˆ—è¡¨
        num_processes: å¹¶è¡Œè¿›ç¨‹æ•°
    Returns:
        vocab, merges: è¯æ±‡è¡¨å’Œåˆå¹¶åˆ—è¡¨
    """
    # å‚æ•°éªŒè¯
    base_vocab_size = 256 + len(special_tokens)
    if vocab_size < base_vocab_size:
        raise ValueError(f"vocab_sizeè‡³å°‘éœ€{base_vocab_size}")

    # 1. å­—èŠ‚åˆ°Unicodeæ˜ å°„
    bytes_to_unicode_map = gpt2_bytes_to_unicode_local()
    unicode_to_bytes_map = {v: bytes([k]) for k, v in bytes_to_unicode_map.items()}

    # 2. åˆå§‹åŒ–è¯æ±‡è¡¨
    vocab = {i: bytes([i]) for i in range(256)}
    next_token_id = 256
    existing_bytes = set(vocab.values())

    # 3. æ·»åŠ ç‰¹æ®Štoken
    for st in special_tokens:
        st_bytes = st.encode("utf-8")
        if st_bytes not in existing_bytes and len(vocab) < vocab_size:
            vocab[next_token_id] = st_bytes
            existing_bytes.add(st_bytes)
            next_token_id += 1

    # 4. å¹¶è¡Œé¢„åˆ†è¯
    print(f"ğŸ“– æ–‡æ¡£æ•°é‡: {len(documents):,}")
    print("é¢„åˆ†è¯è°ƒç”¨çº¿ç¨‹æ•°:", num_processes)
    sequences = parallel_pre_tokenize(documents, num_processes, bytes_to_unicode_map)
    print(f"âœ… é¢„åˆ†è¯å®Œæˆï¼Œå¾—åˆ° {len(sequences):,} ä¸ªtokenåºåˆ—")

    # 5. åˆå§‹åŒ–ç´¢å¼•ç»“æ„
    print("ğŸ”§ æ„å»ºBPEç´¢å¼•...")
    bpe_index = BPEIndex(sequences)
    merges = []
    vocab_progress = len(vocab)
    total_merges = vocab_size - vocab_progress

    # 6. BPEè®­ç»ƒä¸»å¾ªç¯
    print(f"ğŸ”„ å¼€å§‹BPEè®­ç»ƒï¼Œç›®æ ‡åˆå¹¶æ•°: {total_merges:,}")
    progress_bar = tqdm(
        total=total_merges, desc="è®­ç»ƒBPE", unit="åˆå¹¶", mininterval=0.5
    )

    while vocab_progress < vocab_size:
        best_pair = bpe_index.get_most_frequent()
        if best_pair is None:
            print("\nâš ï¸ æ²¡æœ‰æ›´å¤šæœ‰æ•ˆçš„å­—ç¬¦å¯¹å¯ä¾›åˆå¹¶ï¼Œæå‰ç»“æŸè®­ç»ƒ")
            break

        # åˆ›å»ºæ–°token
        new_token_str = best_pair[0] + best_pair[1]
        p1_bytes = unicode_to_bytes_map[best_pair[0]]
        p2_bytes = unicode_to_bytes_map[best_pair[1]]
        new_token_bytes = p1_bytes + p2_bytes

        # æ‰§è¡Œåˆå¹¶
        merge_count = bpe_index.merge_pair(best_pair, new_token_str)
        if merge_count == 0:
            continue

        # æ›´æ–°è¯æ±‡è¡¨
        if new_token_bytes not in existing_bytes:
            vocab[next_token_id] = new_token_bytes
            existing_bytes.add(new_token_bytes)
            merges.append((p1_bytes, p2_bytes))
            next_token_id += 1
            vocab_progress += 1
            progress_bar.update(1)

        # æ›´æ–°æ˜ å°„è¡¨
        unicode_to_bytes_map[new_token_str] = new_token_bytes

    progress_bar.close()
    return vocab, merges


def evaluate_tokenizer(
    vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], test_texts: List[str]
):
    """ç®€å•è¯„ä¼°åˆ†è¯å™¨æ•ˆæœ
    test_texts: ç”¨äºè¯„ä¼°çš„æ–‡æœ¬åˆ—è¡¨ï¼ˆList[str]ï¼‰ï¼Œä¼šå±•ç¤ºç¬¬ä¸€ä¸ªæ ·ä¾‹çš„é¢„è§ˆ
    """
    print("\nğŸ” åˆ†è¯å™¨è¯„ä¼°")
    if test_texts and len(test_texts) > 0:
        first = test_texts[0]
        sample_text = first[:200] + "..." if len(first) > 200 else first
    else:
        sample_text = ""
    print(f"æ ·ä¾‹æ–‡æœ¬: {sample_text}")
    # æ›´è¯¦å°½çš„ç»Ÿè®¡ä¸é‡å¤æ£€æŸ¥
    import statistics

    unique_tokens = set(vocab.values())
    vocab_size = len(vocab)
    unique_count = len(unique_tokens)

    # æ‰¾å‡ºä¸åŒ id æŒ‡å‘ç›¸åŒ bytes çš„æƒ…å†µ
    by_bytes = defaultdict(list)
    for idx, token_bytes in vocab.items():
        by_bytes[token_bytes].append(idx)

    duplicates = {b: ids for b, ids in by_bytes.items() if len(ids) > 1}

    lengths = [len(b) for b in by_bytes.keys()] if by_bytes else []
    min_len = min(lengths) if lengths else 0
    max_len = max(lengths) if lengths else 0
    avg_len = statistics.mean(lengths) if lengths else 0

    print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size:,}")
    print(f"å”¯ä¸€tokenæ•°: {unique_count:,}")
    print(f"é‡å¤ token æ•°: {len(duplicates):,}")
    print(f"token å­—èŠ‚é•¿åº¦: min={min_len}, avg={avg_len:.2f}, max={max_len}")
    print(f"åˆå¹¶æ“ä½œæ•°: {len(merges):,}")

    # è‹¥å­˜åœ¨é‡å¤ï¼Œåˆ—ä¸¾å°‘é‡ç¤ºä¾‹ï¼ˆhex + è§£ç æ›¿ä»£æ˜¾ç¤ºï¼‰
    if duplicates:
        print("\nç¤ºä¾‹é‡å¤ tokenï¼ˆæœ€å¤š 10 æ¡ï¼‰ï¼š")
        for b, ids in list(duplicates.items())[:10]:
            decoded = b.decode("utf-8", errors="replace")
            print(f" ids={ids}  bytes_hex={b.hex()}  decoded={decoded}")

    # å±•ç¤ºè‹¥å¹²ç¤ºä¾‹ tokenï¼Œä¾¿äºäººå·¥å¿«é€Ÿæ£€æŸ¥
    print("\nç¤ºä¾‹ tokenï¼ˆæŒ‰ id å‡åºï¼Œæœ€å¤š 20ï¼‰ï¼š")
    for idx in sorted(vocab)[:20]:
        b = vocab[idx]
        print(f" {idx}: hex={b.hex()}  decoded={b.decode('utf-8', errors='replace')}")

    # è¿”å›ç»Ÿè®¡ç»“æœï¼Œä¾¿äº programmatic ä½¿ç”¨æˆ–æµ‹è¯•
    stats = {
        "vocab_size": vocab_size,
        "unique_tokens": unique_count,
        "duplicate_count": len(duplicates),
        "lengths": {"min": min_len, "avg": avg_len, "max": max_len},
    }
    return stats


if __name__ == "__main__":
    import sys

    if not hasattr(sys.modules["__main__"], "__spec__"):
        sys.modules["__main__"].__spec__ = None

    # é…ç½®å‚æ•°
    config = {
        "vocab_size": global_config.config["vocab_size"],
        "special_tokens": global_config.config["special_tokens"],
        "num_processes": max(1, MAX_PROCESSES - 1),
        # ä»OpenWebTextä¸­é‡‡æ ·è®­ç»ƒæ–‡æ¡£æ•°
        "train_sample_size": global_config.config["train_sample_size"],
        "valid_sample_size": global_config.config["valid_sample_size"],
    }
    # ========== 1. ä½¿ç”¨ Hugging Face OpenWebText æ•°æ®é›† ==========
    print("ğŸš€ åŠ è½½ OpenWebText æ•°æ®é›†...")
    dataset = load_dataset(
        "sytelus/openwebtext", split=f"train[:{config['train_sample_size']}]"
    )

    # å‡†å¤‡è®­ç»ƒæ–‡æ¡£
    train_docs = prepare_documents_from_dataset(
        dataset,
        split="train",
        sample_size=config["train_sample_size"],
        text_column="text",
    )

    # å‡†å¤‡éªŒè¯æ–‡æ¡£ï¼ˆä»åŒä¸€ä¸ªæ•°æ®é›†ä¸­å†é‡‡æ ·ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯ï¼‰
    valid_docs = prepare_documents_from_dataset(
        dataset,
        split="valid",
        sample_size=config["valid_sample_size"],
        text_column="text",
    )

    # è®­ç»ƒBPE
    print("\nğŸš€ å¼€å§‹è®­ç»ƒï¼ˆOpenWebTextï¼‰")
    start_time = time.time()
    train_vocab, train_merges = run_train_bpe(
        train_docs,
        vocab_size=config["vocab_size"],
        special_tokens=config["special_tokens"],
        num_processes=config["num_processes"],
    )
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {time.time() - start_time:.2f}ç§’")

    # éªŒè¯
    print("\nğŸ”¬ å°è§„æ¨¡éªŒè¯ï¼ˆOpenWebTextï¼‰")
    valid_vocab, valid_merges = run_train_bpe(
        valid_docs,
        vocab_size=config["vocab_size"],
        special_tokens=config["special_tokens"],
        num_processes=config["num_processes"],
    )
    # åˆ†æç»“æœ
    print("\nğŸ“Š è®­ç»ƒç»“æœ")
    print(f"è®­ç»ƒè¯æ±‡è¡¨å¤§å°: {len(train_vocab):,}")
    print(f"è®­ç»ƒåˆå¹¶æ“ä½œæ•°: {len(train_merges):,}")
    print(f"éªŒè¯è¯æ±‡è¡¨å¤§å°: {len(valid_vocab):,}")
    print(f"éªŒè¯åˆå¹¶æ“ä½œæ•°: {len(valid_merges):,}")

    # æ¯”è¾ƒè¯æ±‡è¡¨é‡å ç‡
    train_tokens = set(train_vocab.values())
    valid_tokens = set(valid_vocab.values())
    overlap = train_tokens & valid_tokens
    print(f"\nğŸ“ˆ è¯æ±‡è¡¨é‡å ç‡: {len(overlap)/len(train_tokens):.1%}")

    # åŠ è½½éªŒè¯é›†æ ·ä¾‹è¿›è¡Œè¯„ä¼°
    evaluate_tokenizer(train_vocab, train_merges, valid_docs)

    import json  # éœ€è¦å¯¼å…¥jsonæ¨¡å—

    # åœ¨mainå‡½æ•°æœ«å°¾æ·»åŠ ä»¥ä¸‹ä»£ç ï¼ˆåœ¨å†…å­˜åˆ†æä¹‹å‰ï¼‰
    def save_vocab_and_merges(
        vocab: Dict[int, bytes],
        merges: List[Tuple[bytes, bytes]],
        vocab_path: str,
        merges_path: str,
    ):
        """ä¿å­˜è¯æ±‡è¡¨å’Œåˆå¹¶åˆ—è¡¨åˆ°æ–‡ä»¶"""
        # 1. ä¿å­˜è¯æ±‡è¡¨ (JSONæ ¼å¼)
        vocab_str = {
            idx: token.decode("utf-8", errors="replace") for idx, token in vocab.items()
        }
        with open(vocab_path, "w", encoding="utf-8") as f:
            json.dump(vocab_str, f, ensure_ascii=False, indent=2)

        #!ä¿å­˜ä¸º.txtæ ¼å¼ä¸å®‰å…¨
        # 2. ä¿å­˜åˆå¹¶åˆ—è¡¨ (base64æ ¼å¼)
        with open(merges_path, "w", encoding="utf-8") as f:
            for t1, t2 in merges:
                # å°†bytesç¼–ç ä¸ºBase64å­—ç¬¦ä¸²ï¼ˆASCIIï¼‰
                b64_t1 = base64.b64encode(t1).decode("ascii")
                b64_t2 = base64.b64encode(t2).decode("ascii")
                f.write(f"{b64_t1} {b64_t2}\n")

    # åœ¨mainå‡½æ•°ä¸­è°ƒç”¨ä¿å­˜åŠŸèƒ½ï¼ˆåœ¨è®­ç»ƒå®Œæˆåï¼‰
    output_dir = global_config.config["data_dir"]
    os.makedirs(output_dir, exist_ok=True)

    vocab_path = os.path.join(output_dir, "gpt2_vocab.json")
    merges_path = os.path.join(output_dir, "gpt2_merges.txt")

    save_vocab_and_merges(train_vocab, train_merges, vocab_path, merges_path)
    print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜è‡³: {vocab_path}")
    print(f"âœ… åˆå¹¶åˆ—è¡¨å·²ä¿å­˜è‡³: {merges_path}")

    # å†…å­˜åˆ†æ
    import psutil

    process = psutil.Process()
    mem_usage = process.memory_info().rss / (1024**3)  # GB
    print(f"ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: {mem_usage:.2f} GB")
