# ç›´æ¥å¯¼å…¥ç¼–ç åçš„æ•°æ®
from config import *
from utilities import *
from tqdm import tqdm

import pickle
import os
import glob
import random
import torch
import torch.nn.utils as nn_utils

# æå‡ cudnn æ€§èƒ½ï¼ˆè‹¥è¾“å…¥å°ºå¯¸æ’å®šï¼‰
try:
    torch.backends.cudnn.benchmark = True
except Exception:
    pass


def atomic_save(state, path):
    """åŸå­åŒ–ä¿å­˜ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶å†æ›¿æ¢"""
    tmp_path = path + ".tmp"
    torch.save(state, tmp_path)
    os.replace(tmp_path, path)


def find_latest_checkpoint(dir_path):
    """ä¼˜å…ˆä½¿ç”¨ final/latest.pthï¼›å¦åˆ™æŒ‰ä¿®æ”¹æ—¶é—´é€‰æ‹©æœ€è¿‘çš„ model_epoch_*.pth"""
    final = os.path.join(dir_path, "model_final_*.pth")
    final_files = glob.glob(final)
    if final_files:
        print(f"ä½¿ç”¨ final æ¨¡å‹{final_files[0]}")
        final_files.sort(key=os.path.getmtime, reverse=True)
        return final_files[0]

    latest = os.path.join(dir_path, "latest.pth")
    if os.path.exists(latest):
        print(f"ä½¿ç”¨æœ€æ–°è®­ç»ƒæ¨¡å‹{latest}")
        return latest

    interrupt = os.path.join(dir_path, "*.pth")
    interrupt_files = glob.glob(interrupt)
    if interrupt_files:
        print(f"ä½¿ç”¨æœ€åä¸€æ¬¡ä¸­æ–­è®­ç»ƒæ¨¡å‹{interrupt_files[0]}")
        return interrupt_files[0]

    # å¦åˆ™æŸ¥æ‰¾å…¶ä»– checkpoint
    files = glob.glob(os.path.join(dir_path, "model_epoch_*.pth"))
    if not files:
        print(f"æ— è®­ç»ƒæ¨¡å‹")
        return None
    files.sort(key=os.path.getmtime, reverse=True)
    print(f"ä½¿ç”¨å…¶å®ƒè®­ç»ƒæ¨¡å‹{final_files}")
    return files[0]


def save_checkpoint(
    epoch: int,
    step_in_epoch: int,
    global_step: int,
    model: nn.Module,
    optimizer: optim.Optimizer,
    lr_scheduler,
    path: str,
    timestamp=None,
):
    state = {
        "epoch": epoch,
        "step_in_epoch": step_in_epoch,
        "global_step": global_step,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "timestamp": timestamp or time.strftime("%Y%m%d_%H%M%S"),
        # å°è¯•ä¿å­˜ lr_scheduler çŠ¶æ€ï¼ˆå¦‚æœæ”¯æŒï¼‰
        "lr_scheduler_state": (
            lr_scheduler.state_dict() if hasattr(lr_scheduler, "state_dict") else None
        ),
        # ä¿å­˜éšæœºçŠ¶æ€ä»¥å°½é‡ä¿è¯å¯é‡å¤æ€§
        "torch_rng_state": torch.get_rng_state(),
        "numpy_rng_state": np.random.get_state(),
        "random_state": random.getstate(),
    }
    if torch.cuda.is_available():
        # ä¿å­˜ CUDA å¤šå¡çš„ rng çŠ¶æ€
        try:
            state["cuda_rng_state_all"] = torch.cuda.get_rng_state_all()
        except Exception:
            state["cuda_rng_state_all"] = None

    # åŸå­åŒ–å†™å…¥ç›®æ ‡è·¯å¾„ï¼Œå¹¶æ›´æ–° latest.pth
    checkpoint_dir = config.get("checkpoint_dir", "./checkpoints")
    atomic_save(state, path)
    latest_path = os.path.join(checkpoint_dir, "latest.pth")
    # ä½¿ç”¨ replace ä¿æŒ atomic
    atomic_save(state, latest_path)


def load_checkpoint_if_exists(
    model: nn.Module, optimizer: optim.Optimizer, lr_scheduler
) -> dict | None:
    checkpoint_dir = config.get("checkpoint_dir", "./checkpoints")
    ckpt_path = find_latest_checkpoint(checkpoint_dir)
    if ckpt_path is None:
        return None

    try:
        # å°† numpy çš„é‡æ„å‡½æ•°æ·»åŠ åˆ°å®‰å…¨ç™½åå•
        torch.serialization.add_safe_globals(
            [
                np._core.multiarray._reconstruct,
                np.ndarray,
                np.dtype,
                np.dtypes.UInt32DType,
            ]
        )
    except Exception:
        pass

    try:
        ckpt = torch.load(ckpt_path, map_location=device, weights_only=True)
    except Exception:
        # å¦‚æœ weights_only å¤±è´¥ä¸”æ–‡ä»¶å¯ä¿¡ï¼Œå›é€€åˆ°å®Œæ•´åŠ è½½
        ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)

    model.load_state_dict(ckpt["model_state_dict"])
    optimizer.load_state_dict(ckpt["optimizer_state_dict"])
    if ckpt.get("lr_scheduler_state") is not None and hasattr(
        lr_scheduler, "load_state_dict"
    ):
        try:
            lr_scheduler.load_state_dict(ckpt["lr_scheduler_state"])
        except Exception:
            # æŸäº›è°ƒåº¦å™¨å¯èƒ½æ— æ³•å®Œå…¨æ¢å¤ï¼Œè¿™é‡Œå®¹é”™
            pass
    # æ¢å¤éšæœºæ•°çŠ¶æ€
    if "torch_rng_state" in ckpt:
        try:
            torch.set_rng_state(ckpt["torch_rng_state"])
        except Exception:
            pass
    if (
        "cuda_rng_state_all" in ckpt
        and ckpt["cuda_rng_state_all"] is not None
        and torch.cuda.is_available()
    ):
        try:
            torch.cuda.set_rng_state_all(ckpt["cuda_rng_state_all"])
        except Exception:
            pass
    if "numpy_rng_state" in ckpt:
        np.random.set_state(ckpt["numpy_rng_state"])
    if "random_state" in ckpt:
        random.setstate(ckpt["random_state"])
    return ckpt


def train():

    train_encode_ids_path = config.get("train_encode_ids_path")
    valid_encode_ids_path = config.get("valid_encode_ids_path")

    if not train_encode_ids_path:
        raise ValueError("â—é…ç½®ä¸­ç¼ºå°‘ 'train_encode_ids_path'")
    if not valid_encode_ids_path:
        raise ValueError("â—é…ç½®ä¸­ç¼ºå°‘ 'valid_encode_ids_path'")

    try:
        with open(train_encode_ids_path, "rb") as f:
            train_encode_ids = pickle.load(f)
        with open(valid_encode_ids_path, "rb") as f:
            valid_encode_ids = pickle.load(f)

    except (FileNotFoundError, PermissionError, ValueError, RuntimeError) as e:
        print(f"âŒ å·²åˆ†è¯ tokens åŠ è½½å¤±è´¥\n: {e}")
        # æ ¹æ®ä½ çš„éœ€æ±‚å†³å®šæ˜¯é€€å‡ºç¨‹åºè¿˜æ˜¯è¿”å›ç©ºæ•°æ®
        import sys

        sys.exit(1)  # é€€å‡ºç¨‹åº
        # æˆ–è€…è®¾ç½®ç©ºçš„æ•°æ®åŠ è½½å™¨
        # train_data_loader = None
        # valid_data_loader = None

    train_data_loader = DataLoader(
        train_encode_ids, config["batch_size"], config["context_length"], shuffle=True
    )
    valid_data_loader = DataLoader(
        valid_encode_ids, config["batch_size"], config["context_length"], shuffle=True
    )
    print(
        f"âœ…å·²åŠ è½½è®­ç»ƒæ•°æ®: {len(train_encode_ids)} tokens, éªŒè¯æ•°æ®: {len(valid_encode_ids)} tokens"
    )

    # åŠ è½½æ¨¡å‹
    model = TransformerModule(
        config["d_model"],
        config["n_heads"],
        config["d_ff"],
        config["context_length"],
        config["rope_theta"],
        config["n_layers"],
        config["vocab_size"],
        device,
    ).to(device)
    # ç¼–è¯‘æ¨¡å‹ï¼ˆä»…å½“ PyTorch ç‰ˆæœ¬æ”¯æŒä¸”éœ€è¦æ—¶ï¼‰
    try:
        model = torch.compile(model)
        print("ç¼–è¯‘æ¨¡å‹æˆåŠŸ")
    except Exception as e:
        print(f"ç¼–è¯‘æ¨¡å‹å¤±è´¥, ä½¿ç”¨åŸå§‹æ¨¡å‹. Error: {e}")

    # åŠ è½½ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = CosineSchedule(
        config["max_learning_rate"],
        config["min_learning_rate"],
        config["lr_warmup_steps"],
        config["cosine_cycle_iters"],
    )
    optimizer = AdamW(
        model.parameters(),
        config["initial_lr"],
        (config["adam_beta1"], config["adam_beta2"]),
        config["eps"],
        config["weight_decay"],
    )

    # AMP ä¸æ¢¯åº¦ç´¯ç§¯è®¾ç½®
    use_amp = bool(config.get("use_amp", True)) and torch.cuda.is_available()
    grad_accum_steps = int(config.get("grad_accum_steps", 1))
    # GradScaler: prefer new location `torch.amp.GradScaler` if available,
    # otherwise fall back to `torch.cuda.amp.GradScaler` for older torch versions.
    GradScalerCls = None
    if hasattr(torch, "amp") and hasattr(torch.amp, "GradScaler"):
        GradScalerCls = torch.amp.GradScaler
    else:
        GradScalerCls = getattr(torch.cuda.amp, "GradScaler", None)

    scaler = GradScalerCls() if (use_amp and GradScalerCls is not None) else None

    # åŠ è½½æŸå¤±å‡½æ•°
    loss_fn = CrossEntropyLoss()

    print("âœ…æ¨¡å‹åŠ è½½å®Œæˆ\n")

    checkpoint_dir = config["checkpoint_dir"]
    os.makedirs(checkpoint_dir, exist_ok=True)

    # ==== æ—¥å¿—æ–‡ä»¶å‡†å¤‡ ====
    log_dir = config["log_dir"]
    os.makedirs(log_dir, exist_ok=True)
    timestamp = time.strftime("%Y%m%d_%H%M%S")

    print(f"ğŸ“…æ—¥å¿—æ—¶é—´æˆ³: {timestamp}")
    print(f"ğŸ’»è®­ç»ƒè®¾å¤‡: {device}")
    print(f"éªŒè¯é—´éš”æ‰¹æ¬¡: {config['val_interval']} epochs")
    print(f"è®­ç»ƒæ‰¹æ¬¡ï¼š{config['epochs']}\n")

    # å¦‚æœæ£€æµ‹åˆ°å·²æœ‰ checkpointï¼Œåˆ‡æ¢ä¸ºæ¢å¤æ¨¡å¼å¹¶å°†æ—¥å¿—ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€
    ckpt = load_checkpoint_if_exists(model, optimizer, lr_scheduler)
    if ckpt is not None:
        resume = True
        resume_epoch = ckpt.get("epoch", 0)
        resume_step_in_epoch = ckpt.get("step_in_epoch", -1)
        global_step = ckpt.get("global_step", 0)
        started_timestamp = ckpt.get("timestamp", timestamp)
        log_mode = "a"  # append
        print(
            f"âœ”ï¸ä» checkpoint æ¢å¤: epoch={resume_epoch}, step_in_epoch={resume_step_in_epoch}, global_step={global_step}"
        )
    else:
        resume = False
        resume_epoch = 0
        resume_step_in_epoch = -1
        global_step = 0
        started_timestamp = timestamp
        log_mode = "w"  # new log
        print("âŒæ²¡æœ‰æ‰¾åˆ° checkpointï¼Œå¼€å§‹æ–°çš„è®­ç»ƒ")

    log_file_path = os.path.join(log_dir, f"training_log_{started_timestamp}.txt")
    log_file = open(log_file_path, log_mode, encoding="utf-8")
    log_file.write(
        f"âœ”ï¸ Training started at {time.strftime('%Y-%m-%d %H:%M:%S')}, resume={resume}\n"
    )
    log_file.flush()

    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡
    model.to(device)

    # ==== è®­ç»ƒä¸»å¾ªç¯ï¼ˆå¸¦æ¢å¤é€»è¾‘ä¸ä¸­æ–­ä¿å­˜ï¼‰ ====
    try:
        model.train()
        print("ğŸš€å¼€å§‹è®­ç»ƒ...\n")
        for epoch in range(resume_epoch, config["epochs"]):
            # å¦‚æœ resume æ—¶ä»åœ¨åŒä¸€ä¸ª epochï¼Œéœ€è¦ä»ä¸Šæ¬¡ step+1 å¼€å§‹
            if resume and epoch == resume_epoch:
                start_step = resume_step_in_epoch + 1
                # å¦‚æœä¸Šæ¬¡ checkpoint å·²ç»å®Œæˆè¯¥ epochï¼ˆä¾‹å¦‚ä¿å­˜æ—¶ step_in_epoch = args.train_steps-1ï¼‰ï¼Œåˆ™ä»0å¼€å§‹å¹¶ä¸” resume=False
                if start_step >= getattr(
                    args, "train_steps", config.get("train_steps", 0)
                ):
                    start_step = 0
            else:
                start_step = 0

            with tqdm(
                range(start_step, config.get("train_steps", 0)),
                desc=f"ğŸ”„Epoch {epoch}",
                unit="step",
            ) as tbar:
                for step in tbar:
                    # æ›´æ–°å­¦ä¹ ç‡
                    new_lr = lr_scheduler(global_step)
                    for param_group in optimizer.param_groups:
                        param_group["lr"] = new_lr
                    x, y = train_data_loader.get_train_batch_data()
                    x = x.to(device)
                    y = y.to(device)

                    # åœ¨ç´¯ç§¯å‘¨æœŸå¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
                    micro_step_index = (step - start_step) % grad_accum_steps
                    if micro_step_index == 0:
                        optimizer.zero_grad()

                    # å‰å‘ä¸åå‘ï¼ˆå¯é€‰ AMPï¼‰
                    with torch.autocast(device_type="cuda", enabled=use_amp):
                        logits = model(x)
                        loss_val = (
                            loss_fn(logits, y)
                            if callable(loss_fn)
                            else loss_fn.forward(logits, y)
                        )

                    # å°† loss å¹³å‡åˆ°æ¯ä¸ªç´¯ç§¯æ­¥éª¤ä¸Š
                    loss = loss_val / float(grad_accum_steps)

                    if scaler is not None:
                        scaler.scale(loss).backward()
                    else:
                        loss.backward()

                    # åœ¨ç´¯ç§¯æ­¥ç»“æŸæ—¶æ›´æ–°å‚æ•°
                    is_last_micro_step = micro_step_index == (grad_accum_steps - 1)
                    is_final_step = step == config.get("train_steps", 0) - 1
                    if is_last_micro_step or is_final_step:
                        # æ¢¯åº¦è£å‰ªï¼ˆåœ¨ unscale ä¹‹åï¼‰
                        if scaler is not None:
                            # unscale (method name differs across versions)
                            unscale_fn = getattr(
                                scaler, "unscale_", getattr(scaler, "unscale", None)
                            )
                            if unscale_fn is not None:
                                unscale_fn(optimizer)

                            nn_utils.clip_grad_norm_(
                                model.parameters(), config.get("grad_clip", 1.0)
                            )

                            try:
                                step_fn = getattr(scaler, "step", None)
                                if step_fn is not None:
                                    step_fn(optimizer)
                                update_fn = getattr(scaler, "update", None)
                                if update_fn is not None:
                                    update_fn()
                            except Exception as e:
                                print(f"AMP step failed: {e}")
                                raise
                        else:
                            nn_utils.clip_grad_norm_(
                                model.parameters(), config.get("grad_clip", 1.0)
                            )
                            optimizer.step()

                        global_step += 1

                    # ä½¿ç”¨æœªç¼©æ”¾çš„ loss_val è¿›è¡Œæ˜¾ç¤º
                    tbar.set_postfix(
                        {"loss": f"{loss_val.item():.6f}", "å­¦ä¹ ç‡": f"{new_lr:.6f}"}
                    )
                    tbar.update()

                    # å®šæœŸæ‰“å°ä¸å†™æ—¥å¿—
                    if step % 100 == 0:
                        log_message = f"Epoch {epoch} Step {step} LR {new_lr:.6f} Loss: {loss_val.item():.6f} (global_step={global_step})"
                        log_file.write(log_message + "\n")
                        log_file.flush()

            # epoch ç»“æŸåå†™ä¸€æ¬¡ epoch å®Œæˆæ—¥å¿—
            log_message = f"Epoch {epoch} completed with loss: {loss_val.item():.6f}"
            print(log_message)
            print(f"ğŸ’¾æ—¥å¿—å·²ä¿å­˜è‡³ğŸ“ {log_file_path}")
            print(f"æ˜¾å­˜åˆ†é…: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
            print(f"æ˜¾å­˜ç¼“å­˜: {torch.cuda.memory_reserved()/1024**3:.2f}GB\n")

            log_file.write(log_message + "\n")
            log_file.flush()

            # ä¿å­˜å‘¨æœŸæ€§ checkpoint
            if (epoch + 1) % config["checkpoint_interval"] == 0:
                print(f"ğŸ› ï¸ä¿å­˜ checkpoint...")
                ckpt_name = os.path.join(
                    checkpoint_dir, f"model_epoch_{epoch}_{started_timestamp}.pth"
                )
                save_checkpoint(
                    epoch,
                    step,
                    global_step,
                    model,
                    optimizer,
                    lr_scheduler,
                    ckpt_name,
                    timestamp=started_timestamp,
                )
                print(f"ğŸ’¾Checkpoint æˆåŠŸä¿å­˜ epoch {epoch} è‡³æ–‡ä»¶ğŸ“ {ckpt_name}\n")
                log_file.write(
                    f"ğŸ’¾Checkpoint æˆåŠŸä¿å­˜ epoch {epoch} è‡³æ–‡ä»¶ {ckpt_name}\n"
                )
                log_file.flush()

            # éªŒè¯
            if (epoch + 1) % config["val_interval"] == 0:
                print(f"ğŸ”å¼€å§‹éªŒè¯...")
                print(f"éªŒè¯{len(valid_data_loader)}")

                model.eval()
                with torch.no_grad():
                    val_loss = 0.0
                    val_steps = 0

                    with tqdm(
                        valid_data_loader.get_valid_batch_data_iter(),
                        desc="ğŸ”éªŒè¯",
                        unit="step",
                    ) as tbar:
                        for x_val, y_val in tbar:
                            x_val = x_val.to(device)
                            y_val = y_val.to(device)
                            logits = model(x_val)
                            loss_val = (
                                loss_fn(logits, y_val)
                                if callable(loss_fn)
                                else loss_fn.forward(logits, y_val)
                            )
                            val_loss += loss_val.item()
                            val_steps += 1
                        avg_val_loss = val_loss / max(1, val_steps)
                        log_message = (
                            f"éªŒè¯ epoch {epoch}: å¹³å‡ loss: {avg_val_loss:.6f}"
                        )
                        log_file.write(log_message + "\n")
                        log_file.flush()

                        tbar.set_postfix({"å¹³å‡éªŒè¯ loss": f"{avg_val_loss:.6f}"})
                        tbar.update()
                    print(f"âœ…éªŒè¯å®Œæˆ\n")
                model.train()

        # è®­ç»ƒå®Œå…¨ç»“æŸï¼Œä¿å­˜ final checkpoint
        final_ckpt = os.path.join(
            checkpoint_dir, f"model_final_{started_timestamp}.pth"
        )
        save_checkpoint(
            config["epochs"] - 1,
            args.train_steps - 1,
            global_step,
            model,
            optimizer,
            lr_scheduler,
            final_ckpt,
            timestamp=started_timestamp,
        )
        log_file.write("è®­ç»ƒç»“æŸâœ…. Final checkpoint å·²ä¿å­˜è‡³: " + final_ckpt + "\n")
        print("è®­ç»ƒç»“æŸâœ…. Final checkpoint å·²ä¿å­˜è‡³: ", final_ckpt)

    except KeyboardInterrupt:
        # æ•è· Ctrl+C ç­‰ä¸­æ–­ï¼Œä¿å­˜ä¸€ä¸ªä¸­æ–­æ—¶çš„ checkpoint
        interrupt_ckpt = os.path.join(
            checkpoint_dir,
            f"interrupt_epoch_{epoch}_step_{step}_{started_timestamp}.pth",
        )
        save_checkpoint(
            epoch,
            step,
            global_step,
            model,
            optimizer,
            lr_scheduler,
            interrupt_ckpt,
            timestamp=started_timestamp,
        )
        msg = f"ç”¨æˆ·ä¸»åŠ¨ä¸­æ–­è®­ç»ƒ. Checkpoint ä¿å­˜è‡³ğŸ“ {interrupt_ckpt}\n"
        print(msg)
        log_file.write(msg)
        log_file.flush()
        raise  # å¯é€‰ï¼šé‡æ–°æŠ›å‡ºä»¥ä¾¿å¤–éƒ¨çŸ¥æ™“ä¸­æ–­

    except Exception as e:
        # åœ¨å‘ç”Ÿæœªæ•æ‰å¼‚å¸¸æ—¶ä¹Ÿä¿å­˜ checkpointï¼ˆæœ‰åŠ©äºæ’æŸ¥å’Œæ¢å¤ï¼‰
        error_ckpt = os.path.join(
            checkpoint_dir, f"error_epoch_{epoch}_step_{step}_{started_timestamp}.pth"
        )
        try:
            save_checkpoint(
                epoch,
                step,
                global_step,
                model,
                optimizer,
                lr_scheduler,
                error_ckpt,
                timestamp=started_timestamp,
            )
            log_file.write(
                f"Exception occurred: {e}. Checkpoint saved to {error_ckpt}\n"
            )
        except Exception as save_e:
            log_file.write(
                f"Exception occurred: {e}. Failed to save checkpoint: {save_e}\n"
            )
        log_file.flush()
        raise

    finally:
        log_file.write(f"Log closed at {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        log_file.flush()
        log_file.close()


if __name__ == "__main__":
    train()
