import json
import os
import regex
from collections import defaultdict
from config import *
from typing import Dict, List, Tuple, Set, Iterable, Iterator
import base64

# GPT-2é¢„åˆ†è¯æ¨¡å¼
GPT2_SPLIT_PATTERN = (
    r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
)

# é¢„ç¼–è¯‘æ­£åˆ™ä»¥æé«˜é‡å¤è°ƒç”¨æ€§èƒ½
WORD_RE = regex.compile(GPT2_SPLIT_PATTERN, flags=regex.UNICODE)


def gpt2_bytes_to_unicode_local() -> Dict[int, str]:
    """å­—èŠ‚åˆ°Unicodeæ˜ å°„ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
    bs = list(range(33, 127)) + list(range(161, 173)) + list(range(174, 256))
    cs = bs[:]
    n = 0
    for b in range(256):
        if b not in bs:
            bs.append(b)
            cs.append(256 + n)
            n += 1
    return {b: chr(c) for b, c in zip(bs, cs)}


class BPETokenizer:
    def __init__(
        self, config: Dict = None, vocab_path: str = None, merges_path: str = None
    ):
        # åŠ è½½è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™
        if vocab_path is None:
            self.vocab = self._load_vocab(config["vocab_path"])
        else:
            self.vocab = self._load_vocab(vocab_path)
        if merges_path is None:
            self.merges = self._load_merges(config["merges_path"])
        else:
            self.merges = self._load_merges(merges_path)
        self.config = config
        if self.config == None:
            self.config = {
                "special_tokens": ["<|endoftext|>", "<pad>", "<unk>"],
            }

        # åˆ›å»ºåå‘æ˜ å°„(bytes -> ID)
        self.bytes_to_id = {bytes_val: idx for idx, bytes_val in self.vocab.items()}

        # *ç‰¹æ®Štokenå¤„ç†ï¼ˆæ›´é«˜æ•ˆçš„æŸ¥æ‰¾ï¼‰
        self.special_tokens = self.config["special_tokens"]
        self.special_to_id: Dict[str, int] = {}
        for token in self.special_tokens:
            token_bytes = token.encode("utf-8")
            if token_bytes in self.bytes_to_id:
                self.special_to_id[token] = self.bytes_to_id[token_bytes]

        # åˆ›å»ºåˆå¹¶ä¼˜å…ˆçº§æ˜ å°„
        self.merges_priority_map = {pair: i for i, pair in enumerate(self.merges)}

        # å­—èŠ‚åˆ°Unicodeæ˜ å°„ï¼ˆç”¨äºç¼–ç ï¼‰
        self.bytes_to_unicode = gpt2_bytes_to_unicode_local()
        self.unicode_to_bytes = {v: k for k, v in self.bytes_to_unicode.items()}

        # ç¼“å­˜ï¼šå°†å¸¸è§å•è¯çš„ç¼–ç ç»“æœç¼“å­˜ä¸º token id åˆ—è¡¨ï¼Œæ˜¾è‘—åŠ é€Ÿå¤§å‹æ–‡æœ¬ç¼–ç 
        self._encode_cache: Dict[bytes, List[int]] = {}

    def _load_vocab(self, path: str) -> Dict[int, bytes]:
        """åŠ è½½è¯æ±‡è¡¨æ–‡ä»¶"""
        with open(path, "r", encoding="utf-8") as f:
            vocab_str: dict = json.load(f)
        return {int(idx): token.encode("utf-8") for idx, token in vocab_str.items()}

    def _load_merges(self, path: str) -> List[Tuple[bytes, bytes]]:
        """åŠ è½½åˆå¹¶è§„åˆ™æ–‡ä»¶"""
        merges = []
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
            parts = line.split()
            if len(parts) == 2:
                b64_t1, b64_t2 = parts
                t1 = base64.b64decode(b64_t1)
                t2 = base64.b64decode(b64_t2)
                merges.append((t1, t2))
        return merges

    def _bytes_to_unicode_str(self, byte_seq: bytes) -> str:
        """å°†å­—èŠ‚åºåˆ—è½¬æ¢ä¸ºUnicodeå­—ç¬¦ä¸²ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„æ˜ å°„ï¼‰"""
        return "".join(self.bytes_to_unicode[b] for b in byte_seq)

    def _unicode_str_to_bytes(self, unicode_str: str) -> bytes:
        """å°†Unicodeå­—ç¬¦ä¸²è½¬æ¢å›å­—èŠ‚åºåˆ—"""
        return b"".join(bytes([self.unicode_to_bytes[c]]) for c in unicode_str)

    def _get_bpe_merges(self, piece: bytes) -> List[bytes]:
        """
        å¯¹å­—èŠ‚ç‰‡æ®µè¿›è¡ŒBPEç¼–ç ï¼Œè¿”å›å­—èŠ‚åˆ—è¡¨
        """
        # å°†å­—èŠ‚è½¬æ¢ä¸ºUnicodeå­—ç¬¦ä¸²ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„æ˜ å°„ï¼‰
        unicode_str = self._bytes_to_unicode_str(piece)
        parts = [bytes([self.unicode_to_bytes[c]]) for c in unicode_str]

        while len(parts) > 1:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆå¹¶å¯¹
            pairs = set()
            for i in range(len(parts) - 1):
                pair = (parts[i], parts[i + 1])
                if pair in self.merges_priority_map:
                    pairs.add(pair)

            if not pairs:
                break

            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜çš„åˆå¹¶å¯¹
            best_pair = min(pairs, key=lambda pair: self.merges_priority_map[pair])

            # æ‰§è¡Œåˆå¹¶
            new_parts = []
            i = 0
            while i < len(parts):
                if i < len(parts) - 1 and (parts[i], parts[i + 1]) == best_pair:
                    new_parts.append(parts[i] + parts[i + 1])
                    i += 2
                else:
                    new_parts.append(parts[i])
                    i += 1
            parts = new_parts

        return parts

    def encode(self, text: str) -> List[int]:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºtoken IDåºåˆ—"""
        if not text:
            return []

        # æŒ‰ç‰¹æ®Štokenåˆ†å‰²æ–‡æœ¬
        sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)
        special_token_pattern = "|".join(map(regex.escape, sorted_special_tokens))

        if self.special_tokens:
            # ä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™è¿›è¡Œåˆ†å‰²/å¤„ç†
            chunks = regex.split(f"({special_token_pattern})", text)
        else:
            chunks = [text]

        token_ids = []
        for chunk in chunks:
            if not chunk:
                continue

            if chunk in self.special_tokens:
                # å¤„ç†ç‰¹æ®Štoken
                if chunk in self.special_to_id:
                    token_ids.append(self.special_to_id[chunk])
                else:
                    # å›é€€åˆ°UNKæˆ–ç¬¬ä¸€ä¸ªç‰¹æ®Štoken
                    if "<unk>" in self.special_to_id:
                        token_ids.append(self.special_to_id["<unk>"])
                    elif self.special_to_id:
                        token_ids.append(list(self.special_to_id.values())[0])
                    else:
                        token_ids.append(0)
            else:
                # é¢„åˆ†è¯ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
                words: List[str] = WORD_RE.findall(chunk)
                for word in words:
                    if not word:
                        continue
                    # è·å–å•è¯çš„BPE tokensï¼ˆå¹¶ç¼“å­˜æœ€ç»ˆçš„ token id åˆ—è¡¨ï¼‰
                    word_bytes = word.encode("utf-8")
                    if word_bytes in self._encode_cache:
                        token_ids.extend(self._encode_cache[word_bytes])
                        continue

                    pieces = self._get_bpe_merges(word_bytes)

                    # å°†æ¯ä¸ªpieceè½¬æ¢ä¸ºtoken ID
                    piece_ids: List[int] = []
                    for piece in pieces:
                        if piece in self.bytes_to_id:
                            piece_ids.append(self.bytes_to_id[piece])
                        else:
                            # å¤„ç†æœªçŸ¥token
                            if "<unk>" in self.special_to_id:
                                piece_ids.append(self.special_to_id["<unk>"])
                            elif self.special_to_id:
                                piece_ids.append(list(self.special_to_id.values())[0])
                            else:
                                piece_ids.append(0)

                    # ç¼“å­˜å¹¶è¿½åŠ 
                    self._encode_cache[word_bytes] = piece_ids
                    token_ids.extend(piece_ids)

        return token_ids

    def decode(self, token_ids: List[int]) -> str:
        """å°†token IDåºåˆ—è§£ç ä¸ºæ–‡æœ¬"""
        byte_sequence = b""
        for token_id in token_ids:
            if token_id in self.vocab:
                byte_sequence += self.vocab[token_id]
            else:
                # å¤„ç†æ— æ•ˆtoken ID
                if "<unk>" in self.special_to_id:
                    unk_id = self.special_to_id["<unk>"]
                    if unk_id in self.vocab:
                        byte_sequence += self.vocab[unk_id]
                elif self.special_tokens:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹æ®Štokenä½œä¸ºå›é€€
                    first_special_id = list(self.special_to_id.values())[0]
                    if first_special_id in self.vocab:
                        byte_sequence += self.vocab[first_special_id]

        try:
            return byte_sequence.decode("utf-8", errors="replace")
        except UnicodeDecodeError:
            # æç«¯æƒ…å†µä¸‹çš„å›é€€å¤„ç†
            return byte_sequence.decode("latin1", errors="replace")

    def tokenize(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†è¯ä¸ºtokenå­—ç¬¦ä¸²ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        token_ids = self.encode(text)
        tokens = []
        for token_id in token_ids:
            if token_id in self.vocab:
                try:
                    tokens.append(self.vocab[token_id].decode("utf-8"))
                except UnicodeDecodeError:
                    tokens.append(f"<BYTES:{self.vocab[token_id]}>")
            else:
                tokens.append("<INVALID_TOKEN>")
        return tokens


if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    output_dir = "./Experiments/data"
    vocab_path = os.path.join(output_dir, "gpt2_vocab.json")
    merges_path = os.path.join(output_dir, "gpt2_merges.txt")

    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(vocab_path):
        raise FileNotFoundError(f"è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {vocab_path}")
    if not os.path.exists(merges_path):
        raise FileNotFoundError(f"åˆå¹¶è§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨: {merges_path}")

    print("ğŸš€ åŠ è½½è®­ç»ƒå¥½çš„åˆ†è¯å™¨...")
    tokenizer = BPETokenizer(config, vocab_path, merges_path)
    print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ!")

    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "Wow, that is great",
        "you can eat",
        "This is a test with special tokens: <|endoftext|>",
    ]

    # æ·»åŠ ç‰¹æ®Štokenæµ‹è¯•
    test_texts.append(f"Special token test: {tokenizer.config['special_tokens'][0]}")

    print("\nğŸ” å¼€å§‹åˆ†è¯å™¨æµ‹è¯•...")
    for text in test_texts:
        print(f"\næ–‡æœ¬: {text}")

        # ç¼–ç 
        token_ids = tokenizer.encode(text)
        print(
            f"ç¼–ç  ({len(token_ids)} tokens): {token_ids[:20]}{'...' if len(token_ids) > 20 else ''}"
        )

        # è§£ç 
        decoded_text = tokenizer.decode(token_ids)
        print(f"è§£ç : {decoded_text}")

        # éªŒè¯å¾€è¿”ä¸€è‡´æ€§
        if text == decoded_text:
            print("âœ… å¾€è¿”ä¸€è‡´")
        else:
            print("âš ï¸ å¾€è¿”ä¸ä¸€è‡´")
            print(f"åŸå§‹: {text}")
            print(f"è§£ç : {decoded_text}")

            # æ˜¾ç¤ºå·®å¼‚
            for i, (orig_char, dec_char) in enumerate(zip(text, decoded_text)):
                if orig_char != dec_char:
                    print(
                        f"ä½ç½® {i}: åŸå§‹ '{orig_char}' (U+{ord(orig_char):04X}) vs è§£ç  '{dec_char}' (U+{ord(dec_char):04X})"
                    )
                    break
            else:
                if len(text) != len(decoded_text):
                    print(f"é•¿åº¦ä¸åŒ: åŸå§‹ {len(text)} vs è§£ç  {len(decoded_text)}")

        # æ˜¾ç¤ºå‰10ä¸ªtoken
        tokens = tokenizer.tokenize(text)[:10]
        print(f"Tokenç¤ºä¾‹: {tokens}")

    print("\nâœ… æµ‹è¯•å®Œæˆ!")
