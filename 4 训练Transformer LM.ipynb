{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae3256b",
   "metadata": {},
   "source": [
    "# 4 训练Transformer LM\n",
    "我们现在已经有了通过分词器预处理数据和模型（Transformer）的步骤。接下来需要完成支持训练的所有代码，主要包括以下部分：\n",
    "* 损失函数：需要定义损失函数（交叉熵）。\n",
    "* 优化器：需要定义用于最小化该损失的优化器（AdamW）。\n",
    "* 训练循环：需要构建所有支持性的基础设施，包括加载数据、保存检查点以及管理训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc59058",
   "metadata": {},
   "source": [
    "## 4.1 交叉熵损失\n",
    "\n",
    "[“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”](https://www.bilibili.com/video/BV15V411W7VB/?share_source=copy_web&vd_source=c379ccdab784832c917bb852fa2b0584)\n",
    "\n",
    "长度为 $m$ 的token序列组成一个训练集 $D$，此时定义分布概率 $p_\\theta (x_{i+1} \\mid x_{1:i})$ 表示在给定序列 $x$ 的前 $i$ 个元素 $x_{1:i}$ 条件下，模型预测下一个元素 $x_{i+1}$ 的概率。\n",
    "\n",
    "我们定义标准的交叉熵（负对数似然）损失函数：\n",
    "$$\\ell (\\theta ; D)=\\frac{1}{|D|m}\\sum_{x \\in D}\\sum_{i=1}^{m}-\\log p_\\theta (x_{i+1} \\mid x_{1:i})$$\n",
    "\n",
    "* $\\frac{1}{|D|m}$ 是一个归一化因子。其中 $|D|$ 表示训练集中序列 $D$ 的数量，$m$ 表示每个序列的长度。通过除以 $|D|m$，可以将整个训练集上的损失进行平均，使得不同规模训练集的损失具有可比性。\n",
    "\n",
    "（注意：Transformer 的一次前向传播可以得到所有 $i$ 对应的 $p_{\\theta}(x_{i+1} \\mid x_{1:i})$ 。）\n",
    "\n",
    "具体的：\n",
    "$$\n",
    "p(x_{i+1} \\mid x_{1:i}) = \\text{softmax}(o_i)[x_{i+1}] = \\frac{\\exp(o_i[x_{i+1}])}{\\sum_{a=1}^{\\text{vocab\\_ size}} \\exp(o_i[a])}\n",
    "$$\n",
    "\n",
    "* Transfomer 为每个位置计算 logits 向量 $o_i \\in \\mathbb{R}^{\\text{vocab\\_size}}$\n",
    "* $\\text{softmax}(o_i)[x_{i+1}]$ 表示 softmax 作用于 $o_i$ 向量，并取向量中的 $x_{i+1}$ 对应的概率值\n",
    "\n",
    "\n",
    "实现交叉熵损失时需要特别注意数值稳定性问题，这一点与 softmax 的实现类似。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed1372",
   "metadata": {},
   "source": [
    "#### 问题（cross_entropy）：实现交叉熵损失\n",
    "交付内容：编写一个函数来计算交叉熵损失，该函数接收预测的 logits（$o_i$）和目标值（$x_{i+1}$），并计算交叉熵 $\\ell_i = −\\log \\mathbf{softmax}(o_i)[x_{i+1}]$。你的函数应满足以下要求：\n",
    "\n",
    "* 减去最大值以保证数值稳定性。\n",
    "* 尽可能约去 $\\log$ 和 $\\exp$ 运算，避免数值溢出或下溢。\n",
    "* 能够处理任意的批量（batch）维度，并对 batch 维度求平均后返回结果。\n",
    "与第 3.3 节一样，我们假设批量相关的维度始终位于词汇表维度（vocab_size）之前。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7dfd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def softmax(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: 输入张量 (batch_size, vocab_size)\n",
    "    \n",
    "    数值稳定的 softmax 实现\n",
    "    通过减去最大值防止 exp 溢出\n",
    "    \"\"\"\n",
    "\n",
    "    # x_max (batch_size, 1) \n",
    "    x_max = x.max(dim=dim, keepdim=True)[0]  # 防止 exp(x) 上溢 \n",
    "    x_exp = torch.exp(x - x_max)  # 稳定的指数计算\n",
    "    return x_exp / x_exp.sum(dim=dim, keepdim=True)  # 归一化到概率分布\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self, inputs:torch.Tensor, targets:torch.LongTensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (..., vocab_size)\n",
    "            targets: (..., ) 真实标签索引\n",
    "\n",
    "        初始化交叉熵损失计算器\n",
    "        \"\"\"\n",
    "        self.inputs = inputs  # 模型输出的原始 logits\n",
    "        self.targets = targets  # 真实标签索引 (long tensor)\n",
    "        self.vocab_size = inputs.shape[1]  # 词汇表大小\n",
    "        self.batch_size = inputs.shape[0]  # 批次大小\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        前向计算交叉熵损失\n",
    "        步骤：softmax -> 取真实类概率 -> 负对数求和\n",
    "        \"\"\"\n",
    "        y_pred = softmax(self.inputs, dim=1)  # 对每行做 softmax 得预测概率\n",
    "\n",
    "        # 提取真实标签对应的概率 p = y_pred[i, targets[i]]\n",
    "        p = y_pred[range(self.batch_size), self.targets]\n",
    "\n",
    "        # 计算负对数似然并求和\n",
    "        return -torch.sum(torch.log(p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
