{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae3256b",
   "metadata": {},
   "source": [
    "# 4 训练Transformer LM\n",
    "我们现在已经有了通过分词器预处理数据和模型（Transformer）的步骤。接下来需要完成支持训练的所有代码，主要包括以下部分：\n",
    "* 损失函数：需要定义损失函数（交叉熵）。\n",
    "* 优化器：需要定义用于最小化该损失的优化器（AdamW）。\n",
    "* 训练循环：需要构建所有支持性的基础设施，包括加载数据、保存检查点以及管理训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc59058",
   "metadata": {},
   "source": [
    "## 4.1 交叉熵损失\n",
    "\n",
    "[“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”](https://www.bilibili.com/video/BV15V411W7VB/?share_source=copy_web&vd_source=c379ccdab784832c917bb852fa2b0584)\n",
    "\n",
    "长度为 $m$ 的token序列组成一个训练集 $D$，此时定义分布概率 $p_\\theta (x_{i+1} \\mid x_{1:i})$ 表示在给定序列 $x$ 的前 $i$ 个元素 $x_{1:i}$ 条件下，模型预测下一个元素 $x_{i+1}$ 的概率。\n",
    "\n",
    "我们定义标准的交叉熵（负对数似然）损失函数：\n",
    "$$\\ell (\\theta ; D)=\\frac{1}{|D|m}\\sum_{x \\in D}\\sum_{i=1}^{m}-\\log p_\\theta (x_{i+1} \\mid x_{1:i})$$\n",
    "\n",
    "* $\\frac{1}{|D|m}$ 是一个归一化因子。其中 $|D|$ 表示训练集中序列 $D$ 的数量，$m$ 表示每个序列的长度。通过除以 $|D|m$，可以将整个训练集上的损失进行平均，使得不同规模训练集的损失具有可比性。\n",
    "\n",
    "（注意：Transformer 的一次前向传播可以得到所有 $i$ 对应的 $p_{\\theta}(x_{i+1} \\mid x_{1:i})$ 。）\n",
    "\n",
    "具体的：\n",
    "$$\n",
    "p(x_{i+1} \\mid x_{1:i}) = \\text{softmax}(o_i)[x_{i+1}] = \\frac{\\exp(o_i[x_{i+1}])}{\\sum_{a=1}^{\\text{vocab\\_ size}} \\exp(o_i[a])}\n",
    "$$\n",
    "\n",
    "* Transfomer 为每个位置计算 logits 向量 $o_i \\in \\mathbb{R}^{\\text{vocab\\_size}}$\n",
    "* $\\text{softmax}(o_i)[x_{i+1}]$ 表示 softmax 作用于 $o_i$ 向量，并取向量中的 $x_{i+1}$ 对应的概率值\n",
    "\n",
    "\n",
    "实现交叉熵损失时需要特别注意数值稳定性问题，这一点与 softmax 的实现类似。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed1372",
   "metadata": {},
   "source": [
    "#### 问题（cross_entropy）：实现交叉熵损失\n",
    "交付内容：编写一个函数来计算交叉熵损失，该函数接收预测的 logits（$o_i$）和目标值（$x_{i+1}$），并计算交叉熵 $\\ell_i = −\\log \\mathbf{softmax}(o_i)[x_{i+1}]$。你的函数应满足以下要求：\n",
    "\n",
    "* 减去最大值以保证数值稳定性。\n",
    "* 尽可能约去 $\\log$ 和 $\\exp$ 运算，避免数值溢出或下溢。\n",
    "* 能够处理任意的批量（batch）维度，并对 batch 维度求平均后返回结果。\n",
    "与第 3.3 节一样，我们假设批量相关的维度始终位于词汇表维度（vocab_size）之前。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7dfd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def softmax(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: 输入张量 (batch_size, vocab_size)\n",
    "    \n",
    "    数值稳定的 softmax 实现\n",
    "    通过减去最大值防止 exp 溢出\n",
    "    \"\"\"\n",
    "\n",
    "    # x_max (batch_size, 1) \n",
    "    x_max = x.max(dim=dim, keepdim=True)[0]  # 防止 exp(x) 上溢 \n",
    "    x_exp = torch.exp(x - x_max)  # 稳定的指数计算\n",
    "    return x_exp / x_exp.sum(dim=dim, keepdim=True)  # 归一化到概率分布\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self, inputs:torch.Tensor, targets:torch.LongTensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (..., vocab_size)\n",
    "            targets: (..., ) 真实标签索引\n",
    "\n",
    "        初始化交叉熵损失计算器\n",
    "        \"\"\"\n",
    "        self.inputs = inputs  # 模型输出的原始 logits\n",
    "        self.targets = targets  # 真实标签索引 (long tensor)\n",
    "        self.vocab_size = inputs.shape[1]  # 词汇表大小\n",
    "        self.batch_size = inputs.shape[0]  # 批次大小\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        前向计算交叉熵损失\n",
    "        步骤：softmax -> 取真实类概率 -> 负对数求和\n",
    "        \"\"\"\n",
    "        y_pred = softmax(self.inputs, dim=1)  # 对每行做 softmax 得预测概率\n",
    "\n",
    "        # 提取真实标签对应的概率 p = y_pred[i, targets[i]]\n",
    "        p = y_pred[range(self.batch_size), self.targets]\n",
    "\n",
    "        # 计算负对数似然并求和\n",
    "        return -torch.sum(torch.log(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad182bd",
   "metadata": {},
   "source": [
    "困惑度（Perplexity）是另一个评估模型的参数。\n",
    "\n",
    "对于一个长度为 $m$ 的序列，对应的交叉熵损失 $\\ell_i$ 有:\n",
    "$$\\text{perplexity} = \\exp\\left(\\frac{1}{m} \\sum_{i=1}^m \\ell_i\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0a9cc",
   "metadata": {},
   "source": [
    "## 4.2 随机梯度下降优化器 (SGD)\n",
    "现在我们有了损失函数，接下来将开始探索优化器。最简单的基于梯度的优化器是随机梯度下降（SGD）。我们从随机初始化的参数 $\\theta_0$ 开始。然后对于每一步 $t = 0, \\ldots, T - 1$，执行以下更新：\n",
    "\n",
    "$$\\theta_{t+1} \\leftarrow \\theta_t - \\alpha_t \\nabla L(\\theta_t; B_t)$$\n",
    "\n",
    "其中 $B_t$ 是从数据集 $D$ 中采样的随机批量数据，学习率 $\\alpha_t$ 和批量大小 $|B_t|$ 是超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3c37d",
   "metadata": {},
   "source": [
    "### 4.2.1 Pytorch 中实现SGD\n",
    "要实现我们的优化器，我们将继承 PyTorch 的 `torch.optim.Optimizer` 类。一个 `Optimizer` 子类必须实现两个方法：\n",
    "\n",
    "* `def __init__(self, params, ...)` params 将需要优化的参数集合（或参数组，如果用户想为模型的不同部分使用不同的超参数，例如不同的学习率）。确保将 params 传递给基类的 `__init__` 方法，该方法会将这些参数存储起来以供后续使用。你可以添加额外的参数（例如学习率），并将它们作为字典传递给基类构造函数。\n",
    "* `def step(self)` 执行一次参数更新。在训练循环中，这个方法会在**反向传播**后被调用，因此你可以访问到上一批数据的梯度遍历每个参数张量 `p.grad` 并修改\n",
    "\n",
    "假设从一个初始学习率 $\\alpha$ 开始，有如下迭代过程：\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{t + 1}} \\nabla L(\\theta_t; B_t) $$\n",
    "\n",
    "这是一个变体SGD：\n",
    "\n",
    "我们将该版本实现为 Pytorch Optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e61326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights shape: torch.Size([10, 10])\n",
      "tensor(23.8863, grad_fn=<MeanBackward0>)\n",
      "23.88629722595215\n",
      "22.940401077270508\n",
      "22.29613494873047\n",
      "21.784198760986328\n",
      "21.350692749023438\n",
      "20.970468521118164\n",
      "20.62942123413086\n",
      "20.31871223449707\n",
      "20.032381057739258\n",
      "19.766172409057617\n",
      "19.516935348510742\n",
      "19.282262802124023\n",
      "19.060253143310547\n",
      "18.849388122558594\n",
      "18.648414611816406\n",
      "18.45631217956543\n",
      "18.2722110748291\n",
      "18.095375061035156\n",
      "17.9251708984375\n",
      "17.761056900024414\n",
      "17.60255241394043\n",
      "17.44923973083496\n",
      "17.300748825073242\n",
      "17.156753540039062\n",
      "17.01695442199707\n",
      "16.881092071533203\n",
      "16.748926162719727\n",
      "16.620241165161133\n",
      "16.494840621948242\n",
      "16.372547149658203\n",
      "16.253198623657227\n",
      "16.136638641357422\n",
      "16.02273941040039\n",
      "15.911364555358887\n",
      "15.80240249633789\n",
      "15.695738792419434\n",
      "15.59127426147461\n",
      "15.488916397094727\n",
      "15.388571739196777\n",
      "15.290164947509766\n",
      "15.193614959716797\n",
      "15.098847389221191\n",
      "15.0058012008667\n",
      "14.914406776428223\n",
      "14.824604034423828\n",
      "14.736340522766113\n",
      "14.649557113647461\n",
      "14.564208984375\n",
      "14.480243682861328\n",
      "14.397616386413574\n",
      "14.31628704071045\n",
      "14.236210823059082\n",
      "14.157352447509766\n",
      "14.079673767089844\n",
      "14.003138542175293\n",
      "13.927712440490723\n",
      "13.853363990783691\n",
      "13.780064582824707\n",
      "13.707784652709961\n",
      "13.636492729187012\n",
      "13.56616497039795\n",
      "13.496774673461914\n",
      "13.428297996520996\n",
      "13.360712051391602\n",
      "13.293990135192871\n",
      "13.228117942810059\n",
      "13.163066864013672\n",
      "13.098820686340332\n",
      "13.035357475280762\n",
      "12.972661972045898\n",
      "12.910715103149414\n",
      "12.84950065612793\n",
      "12.788996696472168\n",
      "12.729194641113281\n",
      "12.670073509216309\n",
      "12.61161994934082\n",
      "12.553820610046387\n",
      "12.496660232543945\n",
      "12.440125465393066\n",
      "12.384203910827637\n",
      "12.328882217407227\n",
      "12.274147987365723\n",
      "12.219988822937012\n",
      "12.16639518737793\n",
      "12.113354682922363\n",
      "12.060856819152832\n",
      "12.008890151977539\n",
      "11.95744514465332\n",
      "11.906513214111328\n",
      "11.856083869934082\n",
      "11.806146621704102\n",
      "11.756694793701172\n",
      "11.70771598815918\n",
      "11.659205436706543\n",
      "11.611153602600098\n",
      "11.563549995422363\n",
      "11.51639175415039\n",
      "11.46966552734375\n",
      "11.423367500305176\n",
      "11.377490043640137\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "from typing import Optional\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        \"\"\"\n",
    "        简易SGD优化器，学习率按 sqrt(t+1) 衰减\n",
    "        \"\"\"\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        \"\"\"\n",
    "        执行一次参数更新\n",
    "        支持可选的闭包函数用于重新计算模型\n",
    "        \"\"\"\n",
    "        loss = None if closure is None else closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]  # 当前参数组的学习率\n",
    "\n",
    "            p:nn.Parameter\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # 获取参数状态（用于记录迭代次数）\n",
    "                state: dict[str, any] = self.state[p]\n",
    "                step_count = state.get(\"step_count\", 0)  # 读取当前步数，初始为0\n",
    "\n",
    "                #Tensor 与 Tensor.data 区别在于 Tensor.data 不会被 autograd 追踪\n",
    "                #!不安全：使用 .data 可能引起梯度历史不一致或难以发现的 bug，尤其在复杂训练/微分操作中会导致问题\n",
    "                #grad: torch.Tensor = p.grad.data  # 梯度数据\n",
    "\n",
    "                # 更新参数：权重衰减因子为 1/sqrt(step_count + 1)\n",
    "                #p.data -= lr / math.sqrt(step_count + 1) * grad\n",
    "\n",
    "                # 推荐（安全，不会追踪到 autograd）\n",
    "                with torch.no_grad():\n",
    "                    p -= lr / math.sqrt(step_count + 1) * p.grad\n",
    "\n",
    "                # 步数递增并保存回状态\n",
    "                state[\"step_count\"] = step_count + 1\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# ------------------- 测试代码 -------------------\n",
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))  # 初始化可学习参数\n",
    "opt = SGD([weights], lr=1)  # 创建优化器\n",
    "\n",
    "print(\"Initial weights shape:\", weights.shape)\n",
    "for t in range(100):\n",
    "    opt.zero_grad()  # !清零梯度\n",
    "    loss = (weights**2).mean()  # 定义目标：最小化权重平方均值\n",
    "    print(loss.cpu().item())  # 打印当前损失\n",
    "    loss.backward()  # 反向传播计算梯度\n",
    "    opt.step()  # 更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c8652",
   "metadata": {},
   "source": [
    "## 4.3 AdamW\n",
    "现代语言模型通常使用比SGD更复杂的优化器进行训练。最近广泛使用的优化器大多源自Adam优化器 [Kingma 和 Ba, 2015]。我们将使用AdamW [Loshchilov 和 Hutter, 2019]，这是近期研究中广泛采用的一种优化器。AdamW对Adam进行了改进，通过引入权重衰减（在每次迭代中将参数向0拉近）来提升正则化效果，并且该权重衰减与梯度更新解耦。我们将按照Loshchilov和Hutter [2019]论文中算法2的描述来实现AdamW。\n",
    "\n",
    "AdamW 是有状态的：对每个参数，它会维护其一阶和二阶矩的滑动估计值。因此，AdamW 以额外的内存消耗为代价，换取了更好的训练稳定性和收敛性。除了学习率 α 外，AdamW 还包含一对控制矩估计更新的超参数 (β₁, β₂)，以及一个权重衰减率 λ。通常情况下，(β₁, β₂) 设为 (0.9, 0.999)，但像 LLaMA [Touvron 等, 2023] 和 GPT-3 [Brown 等, 2020] 这样的大语言模型通常使用 (0.9, 0.95)。该算法可表述如下，其中 ϵ 是一个很小的值（例如 10⁻⁸），用于在 v 出现极小值时提升数值稳定性：\n",
    "\n",
    "![alt text](1755076516309_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b598d",
   "metadata": {},
   "source": [
    "#### 问题（adamw）：实现 AdamW\n",
    "提交要求：将 AdamW 优化器实现为 torch.optim.Optimizer 的子类。你的类在 `__init__` 中应接收学习率 α，以及超参数 β₁、β₂、ϵ 和 λ。为了帮助你维护状态，基类 Optimizer 提供了一个字典 self.state，它将 nn.Parameter 对象映射到一个字典，用于存储该参数所需的任何信息（对 AdamW 而言，即一阶和二阶矩估计值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413204c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "\n",
    "class AdamW(optim.Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float,\n",
    "        betas: tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        \"\"\"\n",
    "        Simple AdamW implementation.\n",
    "        \"\"\"\n",
    "        loss = None if closure is None else closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            p: nn.Parameter\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad  # use clean grad tensor\n",
    "                state:dict[str,any]= self.state[p]\n",
    "\n",
    "                # 初始化状态缓存：step, 变量 m, RMS v\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"m\"] = torch.zeros_like(p)  # 一阶矩估计\n",
    "                    state[\"v\"] = torch.zeros_like(p)  # 二阶矩估计\n",
    "\n",
    "                m, v = state[\"m\"], state[\"v\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # 更新变量：m = beta1 * m + (1 - beta1) * grad\n",
    "                m.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                # 更新 RMS：v = beta2 * v + (1 - beta2) * grad^2\n",
    "                v.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # 偏差修正系数\n",
    "                bias_correction1 = 1 - beta1 ** state[\"step\"]\n",
    "                bias_correction2 = 1 - beta2 ** state[\"step\"]\n",
    "                step_size = group[\"lr\"] / bias_correction1\n",
    "\n",
    "                # 分母：sqrt(v / bias_correction2) + eps\n",
    "                denom = (v / bias_correction2).sqrt().add_(group[\"eps\"])\n",
    "                # 在 no_grad 环境中更新参数（避免 autograd 跟踪）\n",
    "                with torch.no_grad():\n",
    "                    p.addcdiv_(-step_size, m, denom)\n",
    "                    # 解耦权重衰减（直接作用于参数，不通过梯度）\n",
    "                    p.add_(p, alpha=-group[\"weight_decay\"] * group[\"lr\"])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3187e5",
   "metadata": {},
   "source": [
    "#### 问题（adamwAccounting）：使用 AdamW 训练的资源核算\n",
    "让我们计算运行 AdamW 所需的内存和计算量。假设所有张量均使用 float32（每个元素占 4 字节）。\n",
    "\n",
    "*  运行 AdamW 所需的峰值内存是多少？请根据参数、激活值（activations）、梯度和优化器状态的内存使用情况分解回答。用 batch_size 和模型超参数（vocab_size、context_length、num_layers、d_model、num_heads）表示你的答案。假设 d_ff = 4 × d_model。\n",
    ">为简化激活值的内存计算，仅考虑以下组件：\n",
    ">* Transformer 块：\n",
    ">* * RMSNorm(s)\n",
    ">* * 多头自注意力子层：QKV 投影、QᵀK 矩阵乘法、softmax、加权求和（value 加权和）、输出投影\n",
    ">* 位置前馈网络（FFN）：W1 矩阵乘法、SiLU 激活、W2 矩阵乘法\n",
    ">* 最终的 RMSNorm\n",
    ">* 输出嵌入（output embedding）\n",
    ">* logits 上的交叉熵损失\n",
    "\n",
    "\n",
    "提交要求：分别给出参数、激活值、梯度和优化器状态的代数表达式，以及总内存的表达式。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff1bde",
   "metadata": {},
   "source": [
    "[AdamW内存与计算资源计算器](./data/compute.html)\n",
    "\n",
    "注意：此处假设**d_ff = 4 × d（前馈层隐藏维度）**\n",
    "\n",
    "$$[ \\text{参数数量:} \\quad P = 2Vd + L(12d^2+2d) + d ] \\\\\n",
    " [ \\text{激活数量:} \\quad A_{total} = L(16Nd + 2BhT^2) + Nd + 2NV ]\\\\\n",
    "  [ \\text{内存分解(float32):} \\quad Mem_{param} = 4P, \\quad Mem_{grad} = 4P, \\quad Mem_{opt} = 8P, \\quad Mem_{act} = 4A_{total} ]\\\\\n",
    "   [ \\text{峰值:} \\quad Mem_{peak} = 16P + 4A_{total} \\quad (\\text{bytes}) ]$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
