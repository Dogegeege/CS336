import os
import heapq
import regex
import time
import random
import multiprocessing
from functools import partial
from tqdm import tqdm
from pathlib import Path
from typing import List, Tuple, Dict, DefaultDict, Any, Union
import mmap
import re
from collections import defaultdict

# GPT-2é¢„åˆ†è¯æ¨¡å¼
GPT2_SPLIT_PATTERN = (
    r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
)
MAX_PROCESSES = multiprocessing.cpu_count()


def load_and_sample_data(
    file_path: str, sample_size: int = 22000, special_token: str = "<|endoftext|>"
) -> str:
    """å†…å­˜æ˜ å°„æ–¹å¼åŠ è½½å¹¶é‡‡æ ·æ–‡æ¡£"""
    try:
        with open(file_path, "r+", encoding="utf-8", errors="ignore") as f:
            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                documents = []
                start = 0
                while start < len(mm):
                    end = mm.find(special_token.encode("utf-8"), start)
                    if end == -1:
                        doc = mm[start:].decode("utf-8", errors="replace").strip()
                        if doc:
                            documents.append(doc)
                        break
                    else:
                        doc = mm[start:end].decode("utf-8", errors="replace").strip()
                        if doc:
                            documents.append(doc)
                    start = end + len(special_token)

                # å¦‚æœæ–‡æ¡£é•¿åº¦è¶…è¿‡é‡‡æ ·å¤§å°ï¼Œåˆ™éšæœºé‡‡æ ·
                if len(documents) > sample_size:
                    documents = random.sample(documents, sample_size)

                return special_token.join(documents)
    except Exception as e:
        raise IOError(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")


def gpt2_bytes_to_unicode_local() -> Dict[int, str]:
    """å­—èŠ‚åˆ°Unicodeæ˜ å°„"""
    # åˆ—è¡¨åŒ…å«ASCIIå¯æ‰“å°å­—ç¬¦ï¼ˆ33-126ï¼‰å’Œæ‰©å±•å­—ç¬¦é›†ï¼ˆ161-172, 174-255ï¼‰ï¼Œè¦†ç›–å¸¸è§å­—ç¬¦å’Œç‰¹æ®Šç¬¦å·ã€‚
    # å…¶ä»–å­—èŠ‚ï¼ˆæ¯”å¦‚ä¸­æ–‡ï¼‰æ˜ å°„åˆ°256åŠä»¥ä¸Šçš„Unicodeç ä½ï¼Œç¡®ä¿æ¯ä¸ªå­—èŠ‚éƒ½æœ‰å”¯ä¸€å¯¹åº”ã€‚

    # TODO: æ‰©å±•ä¸­æ–‡å­—ç¬¦é›†
    bs = list(range(33, 127)) + list(range(161, 173)) + list(range(174, 256))
    cs = bs[:]
    n = 0
    for b in range(256):
        if b not in bs:
            bs.append(b)
            cs.append(256 + n)
            n += 1
    return {b: chr(c) for b, c in zip(bs, cs)}


def pre_tokenize_document(
    doc: str, bytes_to_unicode_map: Dict[int, str]
) -> List[List[str]]:
    """é¢„åˆ†è¯å¤„ç†å•ä¸ªæ–‡æ¡£"""
    # åˆ†å‰²åçš„åŸå§‹å­—ç¬¦ä¸²ç‰‡æ®µï¼Œä¾‹å¦‚ï¼š"hello"ã€" world"ã€"!"ã€"ca n't" ç­‰
    tokens = regex.findall(GPT2_SPLIT_PATTERN, doc, flags=regex.UNICODE)
    sequences = []
    for token in tokens:
        # é€å­—èŠ‚æ˜ å°„ä¸ºUnicodeå­—ç¬¦ï¼Œå°†Unicodeå­—ç¬¦ä¸²ç¼–ç ä¸ºUTF-8å­—èŠ‚åºåˆ—ã€‚

        # å°†å˜é•¿å­—ç¬¦è½¬æ¢ä¸ºå›ºå®š1å­—èŠ‚çš„å•ä½ï¼ˆ0-255èŒƒå›´ï¼‰ã€‚ä¾‹å¦‚ï¼š
        # å­—ç¬¦ä¸² "hello" â†’ å­—èŠ‚åºåˆ— [104, 101, 108, 108, 111]
        # ä¸­æ–‡å­—ç¬¦ä¸² "ä½ å¥½" â†’ å­—èŠ‚åºåˆ— [228, 189, 160, 229, 165, 189]
        # è¿™æ ·ï¼ŒBPEå¯ä»¥ç›´æ¥åœ¨å­—èŠ‚çº§åˆ«è¿›è¡Œåˆå¹¶ï¼Œè€Œä¸éœ€è¦å¤„ç†å˜é•¿ç¼–ç çš„å¤æ‚æ€§ã€‚
        # å­—èŠ‚æ˜¯åŸå­å•ä½ï¼Œæ¯ä¸ªå­—èŠ‚ä»£è¡¨ä¸€ä¸ªå›ºå®šå¤§å°çš„â€œåŸºæœ¬å•å…ƒâ€ã€‚

        # ? ä¸ºä»€ä¹ˆå†æ˜ å°„å›Unicodeï¼Ÿ
        # ç›´æ¥åœ¨å­—èŠ‚ä¸Šæ“ä½œæ—¶ï¼Œå­—èŠ‚å€¼æ˜¯0-255çš„æ•´æ•°ï¼Œä¸ä¾¿äºè§‚å¯Ÿå’Œå¤„ç†ï¼ˆå› ä¸ºè®¸å¤šå­—èŠ‚å¯¹åº”ä¸å¯è§å­—ç¬¦æˆ–æ§åˆ¶å­—ç¬¦ï¼‰ã€‚
        # åœ¨ gpt2_bytes_to_unicode_local å‡½æ•°ä¸­ï¼Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªæ˜ å°„è¡¨ï¼Œå°†æ¯ä¸ªå­—èŠ‚ï¼ˆ0-255ï¼‰æ˜ å°„åˆ°ä¸€ä¸ªå”¯ä¸€çš„Unicodeå­—ç¬¦ï¼š
        # ASCIIå¯æ‰“å°å­—ç¬¦ï¼ˆ33-126ï¼Œå¦‚å­—æ¯ã€æ•°å­—ã€ç¬¦å·ï¼‰ä¿æŒä¸å˜ã€‚
        # å…¶ä»–å­—èŠ‚ï¼ˆå¦‚ä¸å¯è§å­—ç¬¦æˆ–æ‰©å±•å­—ç¬¦ï¼‰æ˜ å°„åˆ°256åŠä»¥ä¸Šçš„Unicodeç ä½ï¼ˆä½¿ç”¨ chr(256 + n)ï¼‰ï¼Œç¡®ä¿æ¯ä¸ªå­—èŠ‚éƒ½æœ‰ä¸€ä¸ªå¯è§çš„ã€å¯æ‰“å°çš„Unicodeä»£ç†å­—ç¬¦ã€‚
        # ä¾‹å¦‚ï¼Œå­—èŠ‚ 0ï¼ˆç©ºå­—ç¬¦ï¼‰å¯èƒ½æ˜ å°„åˆ°æŸä¸ªç‰¹æ®ŠUnicodeå­—ç¬¦ï¼Œå¦‚ 'Ä€' æˆ–ç±»ä¼¼ã€‚
        # åœ¨ pre_tokenize_document ä¸­ï¼Œæ¯ä¸ªå­—èŠ‚è¢«è½¬æ¢ä¸ºå¯¹åº”çš„Unicodeå­—ç¬¦ï¼štoken_unicode = "".join(bytes_to_unicode_map[b] for b in token.encode("utf-8"))
        # è¿™å°†å­—èŠ‚åºåˆ—è½¬æ¢ä¸ºUnicodeå­—ç¬¦ä¸²ï¼Œä½†æ¯ä¸ªâ€œå­—ç¬¦â€å®é™…ä¸Šä»£è¡¨ä¸€ä¸ªå­—èŠ‚ã€‚
        # ä¾‹å¦‚ï¼Œ"hello" çš„å­—èŠ‚ [104, 101, 108, 108, 111] æ˜ å°„ä¸º ['h', 'e', 'l', 'l', 'o']ï¼ˆå¦‚æœ104å¯¹åº”'h'ï¼‰ã€‚
        # *å¯¹äºä¸­æ–‡ï¼Œå­—èŠ‚åºåˆ—ä¼šè¢«æ˜ å°„ä¸ºä¸€ç³»åˆ—å¯è§çš„ä»£ç†å­—ç¬¦ï¼Œä¾¿äºBPEåœ¨è¿™äº›â€œå­—ç¬¦â€ä¸Šè¿›è¡Œåˆå¹¶
        token_unicode = "".join(bytes_to_unicode_map[b] for b in token.encode("utf-8"))
        sequences.append(list(token_unicode))
    # è¿”å›å½¢å¼å½¢å¦‚: [['H', 'e', 'l', 'l', 'o'],[','],[' ', 'w', 'o', 'r', 'l', 'd'],['!']]
    return sequences


def parallel_pre_tokenize(
    documents: List[str], num_processes: int, bytes_to_unicode_map: Dict[int, str]
) -> List[List[str]]:
    """å¹¶è¡Œé¢„åˆ†è¯ä¼˜åŒ–"""
    if num_processes <= 1:
        return [
            seq
            for doc in documents
            for seq in pre_tokenize_document(doc, bytes_to_unicode_map)
        ]

    with multiprocessing.Pool(
        num_processes, initializer=init_worker, initargs=(bytes_to_unicode_map,)
    ) as pool:
        results = list(
            tqdm(
                pool.imap(pre_tokenize_worker, documents, chunksize=50),
                total=len(documents),
                desc="é¢„åˆ†è¯",
                mininterval=1,
            )
        )
    return [seq for doc_sequences in results for seq in doc_sequences]


# å…¨å±€å˜é‡ç”¨äºå¤šè¿›ç¨‹
global_worker_byte_map = None


def init_worker(byte_map: Dict[int, str]):
    global global_worker_byte_map
    global_worker_byte_map = byte_map


def pre_tokenize_worker(doc: str) -> List[List[str]]:
    return pre_tokenize_document(doc, global_worker_byte_map)


class BPEIndex:
    """é«˜æ•ˆç´¢å¼•ç»“æ„ç”¨äºBPEåˆå¹¶"""

    def __init__(self, sequences: List[List[str]]):
        self.sequences = sequences  # å­˜å‚¨æ‰€æœ‰æ–‡æœ¬åºåˆ—
        self.pair_counts: DefaultDict[Tuple[str, str], int] = defaultdict(
            int
        )  # ç»Ÿè®¡å­—èŠ‚å¯¹é¢‘ç‡
        self.pair_positions: DefaultDict[Tuple[str, str], List[Tuple[int, int]]] = (
            defaultdict(list)
        )  # è®°å½•å­—èŠ‚å¯¹ä½ç½®
        self.heap = []  # æœ€å¤§å †ï¼ˆå­˜æœ€é«˜é¢‘å­—èŠ‚å¯¹ï¼‰
        self.heap_entries: Dict[Tuple[str, str], Any] = {}  # å †æ¡ç›®å¿«é€Ÿè®¿é—®

        # åˆå§‹åŒ–ç´¢å¼• ä¸€æ¬¡æ€§ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„å‡ºç°ä½ç½®å’Œé¢‘ç‡â€”â€”å°†ä¸å¯è¡Œçš„O(NÂ²)é—®é¢˜è½¬åŒ–ä¸ºå¯å¤„ç†çš„O(N log N)
        for seq_idx, seq in enumerate(sequences):
            for pos in range(len(seq) - 1):
                pair = (seq[pos], seq[pos + 1])
                self.pair_counts[pair] += 1
                self.pair_positions[pair].append((seq_idx, pos))

        # æ„å»ºå † å°†é«˜é¢‘å­—èŠ‚å¯¹ï¼ˆ>1æ¬¡ï¼‰åŠ å…¥æœ€å¤§å †ï¼Œè®© get_most_frequent() èƒ½ O(1) è·å–æœ€é«˜é¢‘å¯¹ã€‚
        for pair, count in self.pair_counts.items():
            if count > 1:  # åªæ·»åŠ è®¡æ•°å¤§äº1çš„pair
                entry = [-count, pair]
                heapq.heappush(self.heap, entry)  # å †é‡æ„æ•°ç»„ï¼ˆå°æ ¹å †ï¼‰
                self.heap_entries[pair] = entry

    def get_most_frequent(self) -> Tuple[str, str]:
        """å¿«é€Ÿè¿”å›å½“å‰æœ€é«˜é¢‘å­—èŠ‚å¯¹ï¼ˆè·³è¿‡å·²è¢«åˆå¹¶çš„æ— æ•ˆæ¡ç›®ï¼‰"""
        while self.heap:
            neg_count, pair = self.heap[0]
            # æ£€æŸ¥pairæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            if pair not in self.heap_entries:
                heapq.heappop(self.heap)
                continue

            current_count = self.pair_counts.get(pair, 0)

            # æ£€æŸ¥è®¡æ•°æ˜¯å¦åŒ¹é…ä¸”å¤§äº1
            if -neg_count == current_count and current_count > 1:
                return pair
            # å¦åˆ™ç§»é™¤æ— æ•ˆæ¡ç›®
            heapq.heappop(self.heap)
            if pair in self.heap_entries:  # ç¡®ä¿æ¡ç›®å­˜åœ¨
                del self.heap_entries[pair]
        return None

    def merge_pair(self, pair: Tuple[str, str], new_token: str) -> int:
        """åˆå¹¶å­—ç¬¦å¯¹å¹¶æ›´æ–°ç´¢å¼•"""
        if pair not in self.pair_positions or not self.pair_positions[pair]:
            return 0

        # å°†å­—èŠ‚å¯¹æŒ‰åºåˆ—åˆ†ç»„
        positions_by_seq = defaultdict(list)
        for seq_idx, pos in self.pair_positions[pair]:
            positions_by_seq[seq_idx].append(pos)

        merge_count = 0
        # éå†åˆ†ç»„
        for seq_idx, positions in positions_by_seq.items():
            seq = self.sequences[seq_idx]  # æµ…æ‹·è´
            # æŒ‰ä½ç½®å€’åºæ’åº
            positions.sort(reverse=True)
            last_merged_pos = -2

            for pos in positions:
                # æ£€æŸ¥æ˜¯å¦å·²è¢«å‰é¢çš„åˆå¹¶å½±å“
                if pos >= len(seq) - 1 or pos <= last_merged_pos:
                    continue  # è·³è¿‡å·²åˆå¹¶ä½ç½®
                if seq[pos] != pair[0] or seq[pos + 1] != pair[1]:
                    continue  # åªåˆå¹¶å®Œå…¨åŒ¹é…çš„pair

                # æ‰§è¡Œåˆå¹¶
                seq[pos] = new_token
                del seq[pos + 1]
                merge_count += 1
                last_merged_pos = pos

                # æ›´æ–°å·¦ä¾§pair, (A, B) -> (A, new_token)
                if pos > 0:
                    left_pair = (seq[pos - 1], pair[0])
                    self._update_pair_count(left_pair, -1)

                    new_left_pair = (seq[pos - 1], new_token)
                    self._update_pair_count(new_left_pair, 1)
                    self._add_position(new_left_pair, seq_idx, pos - 1)

                # æ›´æ–°å³ä¾§pair , (B, C) -> (new_token, C)
                if pos < len(seq) - 1:
                    right_pair = (pair[1], seq[pos + 1])
                    self._update_pair_count(right_pair, -1)

                    new_right_pair = (new_token, seq[pos + 1])
                    self._update_pair_count(new_right_pair, 1)
                    self._add_position(new_right_pair, seq_idx, pos)

        # æ¸…ç†å·²åˆå¹¶çš„pair
        if pair in self.pair_counts:
            del self.pair_counts[pair]
        if pair in self.pair_positions:
            del self.pair_positions[pair]
        if pair in self.heap_entries:
            # æ ‡è®°ä¸ºæ— æ•ˆï¼Œç¨åæ¸…ç†
            self.heap_entries[pair] = None

        return merge_count

    def _update_pair_count(self, pair: Tuple[str, str], delta: int):
        """æ›´æ–°å­—ç¬¦å¯¹è®¡æ•°
        æ›´æ–°`pair_counts`çš„è®¡æ•°ï¼Œå¹¶ç»´æŠ¤å †ç»“æ„\n

        pair: éœ€è¦æ›´æ–°çš„å­—ç¬¦å¯¹\n
        delta: è®¡æ•°å¢é‡ï¼ˆæ­£æ•°å¢åŠ ï¼Œè´Ÿæ•°å‡å°‘ï¼‰
        """
        if delta == 0:
            return

        # ç¡®ä¿pairå­˜åœ¨äºå­—å…¸ä¸­
        if pair not in self.pair_counts:
            self.pair_counts[pair] = 0

        new_count = self.pair_counts[pair] + delta
        self.pair_counts[pair] = new_count

        # ç¡®ä¿è®¡æ•°ä¸ä¸ºè´Ÿ
        if new_count < 0:
            new_count = 0
            self.pair_counts[pair] = 0

        if pair in self.heap_entries and self.heap_entries[pair] is not None:
            # æ›´æ–°å †æ¡ç›®
            self.heap_entries[pair][0] = -new_count
            heapq.heapify(self.heap)  # è°ƒæ•´å †
        elif new_count > 1:  # åªæ·»åŠ è®¡æ•°å¤§äº1çš„pair
            # æ–°å»ºå †æ¡ç›®
            entry = [-new_count, pair]
            heapq.heappush(self.heap, entry)
            self.heap_entries[pair] = entry

    def _add_position(self, pair: Tuple[str, str], seq_idx: int, pos: int):
        """æ·»åŠ æ–°ä½ç½®åˆ°ç´¢å¼•"""
        self.pair_positions[pair].append((seq_idx, pos))


def run_train_bpe(
    input_path: Union[str, os.PathLike],
    vocab_size: int,
    special_tokens: List[str] = ["<|endoftext|>"],
    num_processes: int = 8,
    sample_size: int = 22000,
    **kwargs,
) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:
    """# è¿è¡ŒBPEè®­ç»ƒæµç¨‹ \n
    input_path: æ•°æ®é›†è·¯å¾„ï¼Œæ”¯æŒâœ… 1. å­—ç¬¦ä¸²è·¯å¾„(str) âœ… 2. è·¯å¾„å¯¹è±¡(å®ç°äº† os.PathLike åè®®çš„å¯¹è±¡)\n
    vocab_size: ç›®æ ‡è¯æ±‡è¡¨å¤§å°\n
    special_tokens: ç‰¹æ®Štokenåˆ—è¡¨\n
    num_processes: å¹¶è¡Œè¿›ç¨‹æ•°\n
    sample_size: é‡‡æ ·æ–‡æ¡£æ•°é‡\n
    è¿”å›: è¯æ±‡è¡¨å’Œåˆå¹¶åˆ—è¡¨
    """
    # å‚æ•°éªŒè¯
    base_vocab_size = 256 + len(special_tokens)
    if vocab_size < base_vocab_size:
        raise ValueError(f"vocab_sizeè‡³å°‘éœ€{base_vocab_size}")

    # 1. å­—èŠ‚åˆ°Unicodeæ˜ å°„<bytes,str>
    bytes_to_unicode_map = (
        gpt2_bytes_to_unicode_local()
    )  # {33: '!', 34: '"', ..., 255: 'Ã¿'}
    unicode_to_bytes_map = {
        v: bytes([k]) for k, v in bytes_to_unicode_map.items()
    }  # {!: b'!', ": b'"', ...,Åƒ: b'Ã¿'}

    # 2. åˆå§‹åŒ–è¯æ±‡è¡¨
    vocab = {i: bytes([i]) for i in range(256)}
    next_token_id = 256
    existing_bytes = set(vocab.values())

    # 3. æ·»åŠ ç‰¹æ®Štoken
    for st in special_tokens:
        st_bytes = st.encode("utf-8")
        if st_bytes not in existing_bytes and len(vocab) < vocab_size:
            vocab[next_token_id] = st_bytes
            existing_bytes.add(st_bytes)
            next_token_id += 1

    # 4. åŠ è½½å¹¶é‡‡æ ·æ•°æ®
    print(f"ğŸ“– ä» {input_path} åŠ è½½å¹¶é‡‡æ · {sample_size} ä¸ªæ–‡æ¡£...")
    # å°†æ–‡æ¡£ç”±UTF-8æ ¼å¼è¯»å–è§£ç ä¸ºå­—ç¬¦ä¸²
    text = load_and_sample_data(input_path, sample_size, special_tokens[0])

    # 5. åˆ†å‰²æ–‡æ¡£

    # re.escape() ä¸æ”¹å˜å­—ç¬¦ä¸²çš„è¯­ä¹‰å†…å®¹ï¼Œä½†ç¡®ä¿å®ƒä»¬åœ¨æ­£åˆ™è¡¨è¾¾å¼ä¸­è¢«å½“ä½œâ€Œå­—é¢å­—ç¬¦ä¸²â€Œå¤„ç†ï¼Œ
    # é¿å…è¢«è§£é‡Šä¸ºæ­£åˆ™å…ƒå­—ç¬¦ï¼Œæ¯”å¦‚re.escape('abcdef') ä¼šè¿”å› 'abc\*def'
    escaped_tokens = [re.escape(st) for st in special_tokens]
    split_pattern = "|".join(escaped_tokens)  # <\\|endoftext\\|>|<pad>|<unk>

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ–‡æœ¬ä¸ºæ–‡æ¡£åˆ—è¡¨
    documents = [part for part in re.split(split_pattern, text) if part]

    # 6. å¹¶è¡Œé¢„åˆ†è¯
    # é¢„åˆ†è¯æ ¼å¼str(UTF-8)å½¢å¦‚: [['H', 'e', 'l', 'l', 'o'],[','],[' ', 'w', 'o', 'r', 'l', 'd'],['!']]
    print("é¢„åˆ†è¯è°ƒç”¨çº¿ç¨‹æ•°:", num_processes)
    sequences = parallel_pre_tokenize(documents, num_processes, bytes_to_unicode_map)
    print(f"âœ… é¢„åˆ†è¯å®Œæˆï¼Œå¾—åˆ° {len(sequences):,} ä¸ªtokenåºåˆ—")

    # 7. åˆå§‹åŒ–ç´¢å¼•ç»“æ„
    print("ğŸ”§ æ„å»ºBPEç´¢å¼•...")
    bpe_index = BPEIndex(sequences)
    merges = []
    vocab_progress = len(vocab)
    total_merges = vocab_size - vocab_progress

    # 8. BPEè®­ç»ƒä¸»å¾ªç¯
    print(f"ğŸ”„ å¼€å§‹BPEè®­ç»ƒï¼Œç›®æ ‡åˆå¹¶æ•°: {total_merges:,}")
    progress_bar = tqdm(
        total=total_merges, desc="è®­ç»ƒBPE", unit="åˆå¹¶", mininterval=0.5
    )

    while vocab_progress < vocab_size:
        best_pair = bpe_index.get_most_frequent()
        if best_pair is None:
            print("\nâš ï¸ æ²¡æœ‰æ›´å¤šæœ‰æ•ˆçš„å­—ç¬¦å¯¹å¯ä¾›åˆå¹¶ï¼Œæå‰ç»“æŸè®­ç»ƒ")
            break

        # åˆ›å»ºæ–°token
        new_token_str = best_pair[0] + best_pair[1]
        p1_bytes = unicode_to_bytes_map[best_pair[0]]
        p2_bytes = unicode_to_bytes_map[best_pair[1]]
        new_token_bytes = p1_bytes + p2_bytes

        # æ‰§è¡Œåˆå¹¶
        merge_count = bpe_index.merge_pair(best_pair, new_token_str)
        if merge_count == 0:
            continue

        # æ›´æ–°è¯æ±‡è¡¨
        if new_token_bytes not in existing_bytes:
            vocab[next_token_id] = new_token_bytes
            existing_bytes.add(new_token_bytes)
            merges.append((p1_bytes, p2_bytes))
            next_token_id += 1
            vocab_progress += 1
            progress_bar.update(1)

        # æ›´æ–°æ˜ å°„è¡¨
        unicode_to_bytes_map[new_token_str] = new_token_bytes

    progress_bar.close()
    return vocab, merges


def evaluate_tokenizer(
    vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], test_text: str
):
    """ç®€å•è¯„ä¼°åˆ†è¯å™¨æ•ˆæœ"""
    print("\nğŸ” åˆ†è¯å™¨è¯„ä¼°")
    sample_text = test_text[:200] + "..." if len(test_text) > 200 else test_text
    print(f"æ ·ä¾‹æ–‡æœ¬: {sample_text}")

    # æ›´è¯¦å°½çš„ç»Ÿè®¡ä¸é‡å¤æ£€æŸ¥
    import statistics

    unique_tokens = set(vocab.values())
    vocab_size = len(vocab)
    unique_count = len(unique_tokens)

    # æ‰¾å‡ºä¸åŒ id æŒ‡å‘ç›¸åŒ bytes çš„æƒ…å†µ
    by_bytes = defaultdict(list)
    for idx, token_bytes in vocab.items():
        by_bytes[token_bytes].append(idx)

    duplicates = {b: ids for b, ids in by_bytes.items() if len(ids) > 1}

    lengths = [len(b) for b in by_bytes.keys()] if by_bytes else []
    min_len = min(lengths) if lengths else 0
    max_len = max(lengths) if lengths else 0
    avg_len = statistics.mean(lengths) if lengths else 0

    print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size:,}")
    print(f"å”¯ä¸€tokenæ•°: {unique_count:,}")
    print(f"é‡å¤ token æ•°: {len(duplicates):,}")
    print(f"token å­—èŠ‚é•¿åº¦: min={min_len}, avg={avg_len:.2f}, max={max_len}")
    print(f"åˆå¹¶æ“ä½œæ•°: {len(merges):,}")

    # è‹¥å­˜åœ¨é‡å¤ï¼Œåˆ—ä¸¾å°‘é‡ç¤ºä¾‹ï¼ˆhex + è§£ç æ›¿ä»£æ˜¾ç¤ºï¼‰
    if duplicates:
        print("\nç¤ºä¾‹é‡å¤ tokenï¼ˆæœ€å¤š 10 æ¡ï¼‰ï¼š")
        for b, ids in list(duplicates.items())[:10]:
            decoded = b.decode("utf-8", errors="replace")
            print(f" ids={ids}  bytes_hex={b.hex()}  decoded={decoded}")

    # å±•ç¤ºè‹¥å¹²ç¤ºä¾‹ tokenï¼Œä¾¿äºäººå·¥å¿«é€Ÿæ£€æŸ¥
    print("\nç¤ºä¾‹ tokenï¼ˆæŒ‰ id å‡åºï¼Œæœ€å¤š 20ï¼‰ï¼š")
    for idx in sorted(vocab)[:20]:
        b = vocab[idx]
        print(f" {idx}: hex={b.hex()}  decoded={b.decode('utf-8', errors='replace')}")

    # è¿”å›ç»Ÿè®¡ç»“æœï¼Œä¾¿äº programmatic ä½¿ç”¨æˆ–æµ‹è¯•
    stats = {
        "vocab_size": vocab_size,
        "unique_tokens": unique_count,
        "duplicate_count": len(duplicates),
        "lengths": {"min": min_len, "avg": avg_len, "max": max_len},
    }
    return stats


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    config = {
        "vocab_size": 10000,
        "special_tokens": ["<|endoftext|>", "<pad>", "<unk>"],
        "num_processes": max(1, MAX_PROCESSES - 1),
        "sample_size": 22000,  # åˆå§‹é‡‡æ ·22,000æ–‡æ¡£
    }

    # æ•°æ®é›†è·¯å¾„
    train_path = "./data/TinyStoriesV2-GPT4-train.txt"
    valid_path = "./data/TinyStoriesV2-GPT4-valid.txt"

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(train_path).exists():
        raise FileNotFoundError(f"è®­ç»ƒé›†æ–‡ä»¶ {train_path} ä¸å­˜åœ¨")
    if not Path(valid_path).exists():
        raise FileNotFoundError(f"éªŒè¯é›†æ–‡ä»¶ {valid_path} ä¸å­˜åœ¨")

    # è®­ç»ƒæ¨¡å‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    start_time = time.time()

    train_vocab, train_merges = run_train_bpe(train_path, **config)

    print(f"\nâœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {time.time() - start_time:.2f}ç§’")

    # å°è§„æ¨¡éªŒè¯ (ä½¿ç”¨éªŒè¯é›†çš„10%)
    print("\nğŸ”¬ å°è§„æ¨¡éªŒè¯")
    valid_config = config.copy()
    valid_config["sample_size"] = int(500)  # éªŒè¯é›†ä½¿ç”¨500æ–‡æ¡£ (10%)

    valid_vocab, valid_merges = run_train_bpe(valid_path, **valid_config)

    # åˆ†æç»“æœ
    print("\nğŸ“Š è®­ç»ƒç»“æœ")
    print(f"è®­ç»ƒè¯æ±‡è¡¨å¤§å°: {len(train_vocab):,}")
    print(f"è®­ç»ƒåˆå¹¶æ“ä½œæ•°: {len(train_merges):,}")
    print(f"éªŒè¯è¯æ±‡è¡¨å¤§å°: {len(valid_vocab):,}")
    print(f"éªŒè¯åˆå¹¶æ“ä½œæ•°: {len(valid_merges):,}")

    # æ¯”è¾ƒè¯æ±‡è¡¨é‡å ç‡
    train_tokens = set(train_vocab.values())
    valid_tokens = set(valid_vocab.values())
    overlap = train_tokens & valid_tokens
    print(f"\nğŸ“ˆ è¯æ±‡è¡¨é‡å ç‡: {len(overlap)/len(train_tokens):.1%}")

    # åŠ è½½éªŒè¯é›†æ ·ä¾‹è¿›è¡Œè¯„ä¼°
    with open(valid_path, "r", encoding="utf-8") as f:
        valid_text = f.read(1000)  # è¯»å–å‰1000å­—ç¬¦ç”¨äºè¯„ä¼°
    evaluate_tokenizer(train_vocab, train_merges, valid_text)

    import json  # éœ€è¦å¯¼å…¥jsonæ¨¡å—

    # åœ¨mainå‡½æ•°æœ«å°¾æ·»åŠ ä»¥ä¸‹ä»£ç ï¼ˆåœ¨å†…å­˜åˆ†æä¹‹å‰ï¼‰
    def save_vocab_and_merges(
        vocab: Dict[int, bytes],
        merges: List[Tuple[bytes, bytes]],
        vocab_path: str,
        merges_path: str,
    ):
        """ä¿å­˜è¯æ±‡è¡¨å’Œåˆå¹¶åˆ—è¡¨åˆ°æ–‡ä»¶"""
        # 1. ä¿å­˜è¯æ±‡è¡¨ (JSONæ ¼å¼)
        vocab_str = {
            idx: token.decode("utf-8", errors="replace") for idx, token in vocab.items()
        }
        with open(vocab_path, "w", encoding="utf-8") as f:
            json.dump(vocab_str, f, ensure_ascii=False, indent=2)

        # 2. ä¿å­˜åˆå¹¶åˆ—è¡¨ (æ–‡æœ¬æ ¼å¼)
        with open(merges_path, "w", encoding="utf-8") as f:
            for merge in merges:
                part1 = merge[0].decode("utf-8", errors="replace")
                part2 = merge[1].decode("utf-8", errors="replace")
                f.write(f"{part1} {part2}\n")

    # åœ¨mainå‡½æ•°ä¸­è°ƒç”¨ä¿å­˜åŠŸèƒ½ï¼ˆåœ¨è®­ç»ƒå®Œæˆåï¼‰
    output_dir = "./out"  # ä¿®æ”¹ä¸ºæ‚¨çš„è¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    vocab_path = os.path.join(output_dir, "gpt2_vocab.json")
    merges_path = os.path.join(output_dir, "gpt2_merges.txt")

    save_vocab_and_merges(train_vocab, train_merges, vocab_path, merges_path)
    print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜è‡³: {vocab_path}")
    print(f"âœ… åˆå¹¶åˆ—è¡¨å·²ä¿å­˜è‡³: {merges_path}")

    # å†…å­˜åˆ†æ
    import psutil

    process = psutil.Process()
    mem_usage = process.memory_info().rss / (1024**3)  # GB
    print(f"ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: {mem_usage:.2f} GB")
